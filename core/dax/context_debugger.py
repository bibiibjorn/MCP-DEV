"""
DAX Context Debugger - Step-by-step context tracking

Provides:
- Step-through evaluation with context tracking
- Context inspection at breakpoints
- Performance profiling per step
- Optimization suggestions
"""

import logging
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple

from .context_analyzer import DaxContextAnalyzer, ContextFlowExplanation

logger = logging.getLogger(__name__)


@dataclass
class EvaluationStep:
    """Represents a single evaluation step"""
    step_number: int
    code_fragment: str
    filter_context: Dict[str, List[Any]]
    row_context: Optional[Dict[str, Any]]
    intermediate_result: Optional[Any]
    explanation: str
    execution_time_ms: Optional[float] = None


@dataclass
class ContextExplanation:
    """Explanation of context at a specific position"""
    position: int
    line: int
    column: int
    filter_context: Dict[str, List[str]]
    row_context: Optional[Dict[str, Any]]
    in_transition: bool
    transition_type: Optional[str]
    explanation: str


@dataclass
class Optimization:
    """Optimization suggestion"""
    severity: str  # "info", "warning", "critical"
    category: str  # "performance", "readability", "correctness"
    message: str
    suggestion: str
    code_example: Optional[str] = None


class DaxContextDebugger:
    """
    Step-by-step DAX debugger with context tracking

    Features:
    - Step through DAX evaluation
    - Track filter and row context at each step
    - Performance profiling
    - Optimization suggestions
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize debugger

        Args:
            config: Optional configuration dict
        """
        self.config = config or {}
        self.max_steps = self.config.get("max_steps", 100)
        self.analyzer = DaxContextAnalyzer(config)

    def step_through(
        self,
        dax_expression: str,
        breakpoints: Optional[List[int]] = None,
        sample_data: Optional[Dict[str, Any]] = None,
    ) -> List[EvaluationStep]:
        """
        Step through DAX evaluation with context tracking

        Args:
            dax_expression: DAX expression to debug
            breakpoints: Optional character positions for breakpoints
            sample_data: Optional sample data for evaluation

        Returns:
            List of evaluation steps
        """
        try:
            steps: List[EvaluationStep] = []

            # First, analyze context transitions
            analysis = self.analyzer.analyze_context_transitions(dax_expression)

            # Create steps from transitions
            for i, transition in enumerate(analysis.transitions):
                step = EvaluationStep(
                    step_number=i + 1,
                    code_fragment=self._extract_code_fragment(
                        dax_expression, transition.location
                    ),
                    filter_context=self._infer_filter_context(transition),
                    row_context=self._infer_row_context(transition),
                    intermediate_result=None,  # Would require actual execution
                    explanation=transition.explanation,
                    execution_time_ms=None,
                )

                steps.append(step)

                # Check if we've reached max steps
                if len(steps) >= self.max_steps:
                    logger.warning(f"Reached max steps limit: {self.max_steps}")
                    break

            logger.info(f"Generated {len(steps)} evaluation steps")

            return steps

        except Exception as e:
            logger.error(f"Error in step-through debugging: {e}", exc_info=True)
            return []

    def explain_context_at_position(
        self,
        dax_expression: str,
        position: int,
    ) -> ContextExplanation:
        """
        Explain context at a specific position in DAX expression

        Args:
            dax_expression: DAX expression
            position: Character position

        Returns:
            ContextExplanation for that position
        """
        try:
            # Analyze entire expression
            analysis = self.analyzer.analyze_context_transitions(dax_expression)

            # Find transitions before this position
            transitions_before = [
                t for t in analysis.transitions if t.location <= position
            ]

            # Get line/column
            lines = dax_expression[:position].split("\n")
            line = len(lines)
            column = len(lines[-1]) + 1

            # Determine if we're inside a transition
            in_transition = False
            transition_type = None

            for t in reversed(transitions_before):
                # Check if position is within this transition's scope
                # (simplified - would need proper parsing)
                if abs(t.location - position) < 50:
                    in_transition = True
                    transition_type = t.type.value
                    break

            # Build explanation
            if in_transition:
                explanation = f"Inside {transition_type} at line {line}. "
                if transitions_before:
                    last_transition = transitions_before[-1]
                    explanation += last_transition.explanation
                else:
                    explanation = f"At position {position}, line {line}."
            else:
                explanation = f"At position {position}, line {line}. "
                if transitions_before:
                    explanation += f"After {len(transitions_before)} context transition(s)."
                else:
                    explanation += "No context transitions detected before this point."

            return ContextExplanation(
                position=position,
                line=line,
                column=column,
                filter_context={},  # Would need runtime data
                row_context=None,
                in_transition=in_transition,
                transition_type=transition_type,
                explanation=explanation,
            )

        except Exception as e:
            logger.error(f"Error explaining context at position: {e}", exc_info=True)
            return ContextExplanation(
                position=position,
                line=0,
                column=0,
                filter_context={},
                row_context=None,
                in_transition=False,
                transition_type=None,
                explanation=f"Error: {str(e)}",
            )

    def suggest_optimizations(
        self,
        context_analysis: ContextFlowExplanation,
    ) -> List[Optimization]:
        """
        Suggest optimizations based on context analysis

        Args:
            context_analysis: Context analysis result

        Returns:
            List of optimization suggestions
        """
        optimizations: List[Optimization] = []

        try:
            # Check for excessive CALCULATE nesting
            if context_analysis.max_nesting_level > 3:
                optimizations.append(
                    Optimization(
                        severity="warning",
                        category="performance",
                        message=f"Excessive CALCULATE nesting ({context_analysis.max_nesting_level} levels)",
                        suggestion="Consider using variables (VAR) to store intermediate calculations and reduce nesting depth.",
                        code_example="""// Instead of:
CALCULATE(
    CALCULATE(
        CALCULATE([Measure], Filter1),
        Filter2
    ),
    Filter3
)

// Use:
VAR Step1 = CALCULATE([Measure], Filter1)
VAR Step2 = CALCULATE(Step1, Filter2)
RETURN CALCULATE(Step2, Filter3)"""
                    )
                )

            # Check for iterator + measure combinations
            iterator_transitions = [
                t for t in context_analysis.transitions
                if t.type.value == "iterator"
            ]

            if len(iterator_transitions) > 3:
                optimizations.append(
                    Optimization(
                        severity="warning",
                        category="performance",
                        message=f"Multiple iterators with measure references ({len(iterator_transitions)} detected)",
                        suggestion="Each measure reference in an iterator causes context transition per row. Consider pre-calculating values using variables or switching to iterators that work with columns.",
                        code_example="""// Instead of:
SUMX(Sales, [Total Sales] * [Tax Rate])

// Use:
SUMX(Sales, Sales[Amount] * Sales[TaxRate])

// Or use variables:
VAR TotalSales = [Total Sales]
VAR TaxRate = [Tax Rate]
RETURN SUMX(Sales, TotalSales * TaxRate)"""
                    )
                )

            # Check complexity score
            if context_analysis.complexity_score > 70:
                optimizations.append(
                    Optimization(
                        severity="critical",
                        category="readability",
                        message=f"High complexity score ({context_analysis.complexity_score}/100)",
                        suggestion="Consider breaking this measure into multiple simpler measures for better maintainability and performance.",
                        code_example=None
                    )
                )

            # Check for implicit measure references
            implicit_refs = [
                t for t in context_analysis.transitions
                if t.type.value == "implicit_measure"
            ]

            if len(implicit_refs) > 10:
                optimizations.append(
                    Optimization(
                        severity="info",
                        category="performance",
                        message=f"Many implicit measure references ({len(implicit_refs)})",
                        suggestion="While implicit CALCULATE is convenient, excessive use can impact performance. Consider whether all references need to be measures.",
                        code_example=None
                    )
                )

            # Add general best practices
            if context_analysis.transitions:
                optimizations.append(
                    Optimization(
                        severity="info",
                        category="correctness",
                        message="Context transition detected",
                        suggestion="Ensure that context transitions are intentional. Use DAX Studio or Tabular Editor to verify query plans and performance.",
                        code_example=None
                    )
                )

            logger.info(f"Generated {len(optimizations)} optimization suggestions")

        except Exception as e:
            logger.error(f"Error generating optimizations: {e}", exc_info=True)

        return optimizations

    def generate_improved_dax(
        self,
        dax_expression: str,
        context_analysis: ContextFlowExplanation,
        anti_patterns: Optional[Dict[str, Any]] = None,
        vertipaq_analysis: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Generate specific DAX improvements with before/after code examples

        Args:
            dax_expression: Original DAX expression
            context_analysis: Context analysis result
            anti_patterns: Optional anti-pattern detection results
            vertipaq_analysis: Optional VertiPaq column analysis results

        Returns:
            Dictionary with specific improvements and rewritten code

        Note:
            This method provides analysis and recommendations. When automatic transformations
            are not possible (requires DAX parser), suggested_code will be None and the AI
            should use the improvements list to generate optimized code.
        """
        improvements = []
        improved_code = None  # Only set if we actually transform the code
        code_actually_transformed = False  # Track if we made real code changes
        transformation_details = []

        try:
            # STEP 1: Apply automatic code transformations using DaxCodeRewriter
            from core.dax.code_rewriter import DaxCodeRewriter

            rewriter = DaxCodeRewriter()
            rewrite_result = rewriter.rewrite_dax(dax_expression)

            if rewrite_result.get('success') and rewrite_result.get('has_changes'):
                improved_code = rewrite_result['rewritten_code']
                code_actually_transformed = True

                # Add rewriter transformations to improvements list
                for trans in rewrite_result.get('transformations', []):
                    improvements.append({
                        "issue": f"Code transformation: {trans['type'].replace('_', ' ').title()}",
                        "severity": "medium",
                        "original_code": trans['original'][:100] + "..." if len(trans['original']) > 100 else trans['original'],
                        "improved_code": trans['transformed'][:100] + "..." if len(trans['transformed']) > 100 else trans['transformed'],
                        "explanation": trans['explanation'],
                        "specific_suggestion": f"Estimated improvement: {trans['estimated_improvement']}"
                    })
                    transformation_details.append(trans['explanation'])

                logger.info(f"Applied {rewrite_result['transformation_count']} automatic transformations")

            # STEP 2: Detect pattern-based improvement opportunities
            # These are detected but not automatically transformed (requires DAX parser)
            # The AI tool will use these recommendations to generate optimized code

            # Detection 1: Iterator with measure references
            iterator_transitions = [
                t for t in context_analysis.transitions
                if t.type.value == "iterator" and t.function in ["SUMX", "AVERAGEX", "COUNTX"]
            ]

            if iterator_transitions:
                improvement = {
                    "issue": "Iterator with measure references causing row-by-row context transitions",
                    "severity": "medium",
                    "original_pattern": "SUMX(Table, [Measure])",
                    "improved_pattern": "SUMX(Table, Table[Column])",
                    "explanation": "Replace measure references inside iterators with direct column references to avoid context transitions in each iteration.",
                    "specific_suggestion": f"Found {len(iterator_transitions)} iterator(s) with potential measure references. Review each iterator to see if measure can be replaced with column reference.",
                    "auto_transform_available": False
                }
                improvements.append(improvement)

            # Detection 2: Excessive CALCULATE nesting
            if context_analysis.max_nesting_level > 2:
                nested_calcs = [
                    t for t in context_analysis.transitions
                    if t.type.value == "explicit_calculate" and t.nested_level > 1
                ]

                if nested_calcs:
                    improvement = {
                        "issue": f"Excessive CALCULATE nesting (depth: {context_analysis.max_nesting_level})",
                        "severity": "high",
                        "original_pattern": "CALCULATE(CALCULATE([Measure], Filter1), Filter2)",
                        "improved_pattern": "CALCULATE([Measure], Filter1, Filter2)\n-- Or use variables:\nVAR Step1 = CALCULATE([Measure], Filter1)\nRETURN CALCULATE(Step1, Filter2)",
                        "explanation": "Use variables (VAR) to flatten nested CALCULATE statements, making code more readable and potentially more efficient.",
                        "specific_suggestion": "Refactor nested CALCULATE statements into sequential variables with a RETURN statement.",
                        "auto_transform_available": False
                    }
                    improvements.append(improvement)

            # Detection 3: Anti-pattern fixes
            if anti_patterns and anti_patterns.get('success') and anti_patterns.get('patterns_detected', 0) > 0:
                for pattern_id, matches in anti_patterns.get('pattern_matches', {}).items():
                    # Find corresponding article
                    article = next((a for a in anti_patterns.get('articles', []) if a['id'] == pattern_id), None)

                    if article and matches:
                        improvement = {
                            "issue": f"Anti-pattern detected: {article['title']}",
                            "severity": "high",
                            "pattern_occurrences": len(matches),
                            "pattern_details": article.get('content', '')[:200] + "...",
                            "explanation": f"This pattern is known to cause performance or correctness issues. Review the article for specific guidance.",
                            "specific_suggestion": f"Found {len(matches)} occurrence(s) of '{pattern_id}'. Apply recommended pattern from DAX documentation.",
                            "auto_transform_available": False
                        }

                        # Add specific code examples for known patterns
                        if 'nested_calculate' in pattern_id.lower():
                            improvement["original_pattern"] = "CALCULATE([Measure], CALCULATE([InnerMeasure], Filter))"
                            improvement["improved_pattern"] = """// Flatten nested CALCULATE using variables
VAR InnerResult = CALCULATE([InnerMeasure], Filter)
VAR FinalResult = CALCULATE(InnerResult, AdditionalFilter)
RETURN FinalResult

// Or combine filters in single CALCULATE
CALCULATE([Measure], Filter1, Filter2)"""

                        elif 'filter_iterator' in pattern_id.lower():
                            improvement["original_pattern"] = "FILTER(Table, [Measure] > 100)"
                            improvement["improved_pattern"] = """// Use KEEPFILTERS or direct column reference
FILTER(Table, Table[Column] > 100)

// Or pre-calculate measure
VAR Threshold = [Measure]
RETURN FILTER(Table, Table[Column] > Threshold)"""

                        improvements.append(improvement)

            # Detection 4: Multiple implicit measure references
            implicit_refs = [
                t for t in context_analysis.transitions
                if t.type.value == "implicit_measure"
            ]

            if len(implicit_refs) > 5:
                improvement = {
                    "issue": f"Multiple implicit measure references ({len(implicit_refs)} detected)",
                    "severity": "medium",
                    "explanation": "Each measure reference creates an implicit CALCULATE wrapper. Consider caching frequently used measures in variables.",
                    "original_pattern": "[Measure1] + [Measure2] + [Measure3]",
                    "improved_pattern": """VAR M1 = [Measure1]
VAR M2 = [Measure2]
VAR M3 = [Measure3]
RETURN M1 + M2 + M3""",
                    "specific_suggestion": "Store measure results in variables when the same measure is referenced multiple times or when measures are used in complex expressions.",
                    "auto_transform_available": False
                }
                improvements.append(improvement)

            # Determine if we have improvements (either actual transformations or recommendations)
            has_improvements = code_actually_transformed or len(improvements) > 0

            return {
                "has_improvements": has_improvements,
                "code_actually_transformed": code_actually_transformed,
                "improvements_count": len(improvements),
                "improvements": improvements,
                "original_code": dax_expression.strip(),
                "suggested_code": improved_code.strip() if code_actually_transformed else None,
                "summary": self._generate_improvement_summary(improvements, code_actually_transformed),
                "transformation_details": transformation_details,
                "note": "Automatic code transformation requires DAX parser. Use the improvements list to guide manual optimization or let the AI generate optimized code." if not code_actually_transformed and has_improvements else None
            }

        except Exception as e:
            logger.error(f"Error generating improved DAX: {e}", exc_info=True)
            return {
                "has_improvements": False,
                "code_actually_transformed": False,
                "improvements_count": 0,
                "improvements": [],
                "error": str(e)
            }

    def _generate_var_based_code(self, dax_expression: str, nested_calcs: List) -> str:
        """
        Generate improved code using variables instead of nested CALCULATE

        This is a simplified version - full implementation would need DAX parsing
        """
        # Simple heuristic-based transformation
        # In production, this would use proper DAX parsing
        if "CALCULATE" not in dax_expression.upper():
            return dax_expression

        # Add comment suggesting variable-based approach
        suggestion = f"""// SUGGESTED IMPROVEMENT: Use variables to reduce nesting
// Original code had {len(nested_calcs)} nested CALCULATE statement(s)

VAR Result =
    // Break down nested CALCULATE into sequential steps
    // Step 1: Apply first filter context
    // Step 2: Apply additional filters
    // Step 3: Final calculation
{dax_expression}

RETURN Result

// NOTE: This is a template. Manually refactor nested CALCULATE statements
// into separate VAR statements for each calculation step."""

        return suggestion

    def _generate_improvement_summary(self, improvements: List[Dict[str, Any]], code_actually_transformed: bool = False) -> str:
        """Generate human-readable summary of improvements"""
        if not improvements:
            return "No significant improvements identified. Your DAX code follows best practices."

        high_severity = sum(1 for i in improvements if i.get('severity') == 'high')
        medium_severity = sum(1 for i in improvements if i.get('severity') == 'medium')
        low_severity = sum(1 for i in improvements if i.get('severity') == 'low')

        if code_actually_transformed:
            summary_parts = [f"Applied {len(improvements)} automatic transformation(s):"]
        else:
            summary_parts = [f"Detected {len(improvements)} improvement opportunity(ies):"]

        if high_severity > 0:
            summary_parts.append(f"  â€¢ {high_severity} high-priority issue(s) - should be addressed")
        if medium_severity > 0:
            summary_parts.append(f"  â€¢ {medium_severity} medium-priority improvement(s) - recommended")
        if low_severity > 0:
            summary_parts.append(f"  â€¢ {low_severity} low-priority enhancement(s) - optional")

        if not code_actually_transformed and improvements:
            summary_parts.append("\nNote: Automatic transformation not available. Use the AI to generate optimized code based on these recommendations.")

        return "\n".join(summary_parts)

    def _extract_code_fragment(self, dax: str, position: int) -> str:
        """Extract code fragment around position"""
        start = max(0, position - 30)
        end = min(len(dax), position + 30)

        fragment = dax[start:end]

        # Add markers
        marker_pos = min(30, position - start)
        return fragment[:marker_pos] + " â–¶ " + fragment[marker_pos:]

    def _infer_filter_context(self, transition) -> Dict[str, List[Any]]:
        """Infer filter context (simplified - would need runtime data)"""
        # This is a placeholder - actual implementation would need
        # to track filter modifications through CALCULATE arguments
        return {}

    def _infer_row_context(self, transition) -> Optional[Dict[str, Any]]:
        """Infer row context (simplified - would need runtime data)"""
        if transition.type.value == "iterator":
            return {"type": "iterator", "function": transition.function}
        return None

    def generate_debug_report(
        self,
        dax_expression: str,
        include_profiling: bool = True,
        include_optimization: bool = True,
        connection_state=None
    ) -> str:
        """
        Generate comprehensive debug report with all enhancements

        Args:
            dax_expression: DAX expression to analyze
            include_profiling: Include performance profiling
            include_optimization: Include optimization suggestions
            connection_state: Optional connection state for advanced analysis

        Returns:
            Formatted debug report
        """
        lines = []

        lines.append("=" * 70)
        lines.append("DAX INTELLIGENCE ENHANCED ANALYSIS REPORT")
        lines.append("=" * 70)
        lines.append("")

        # Context analysis
        analysis = self.analyzer.analyze_context_transitions(dax_expression)

        lines.append("CONTEXT ANALYSIS")
        lines.append("-" * 70)
        lines.append(analysis.summary)
        lines.append("")
        lines.append(f"Complexity Score: {analysis.complexity_score}/100")
        lines.append(f"Max Nesting Level: {analysis.max_nesting_level}")
        lines.append("")

        # Transitions
        if analysis.transitions:
            lines.append("CONTEXT TRANSITIONS")
            lines.append("=" * 70)
            lines.append("")

            for i, t in enumerate(analysis.transitions, 1):
                # Impact icon
                impact_icon = {
                    'low': 'ðŸŸ¢',
                    'medium': 'ðŸŸ¡',
                    'high': 'ðŸ”´',
                    'critical': 'â›”'
                }.get(t.performance_impact.value, 'âšª')

                # Type icon
                type_icon = {
                    'iterator': 'ðŸ”„',
                    'implicit_measure': 'ðŸ“Š',
                    'filter_context': 'ðŸ”',
                    'row_context': 'ðŸ“'
                }.get(t.type.value, 'â€¢')

                lines.append(f"â”Œâ”€ Transition #{i} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                lines.append(f"â”‚ {type_icon} {t.function} (Line {t.line}, Col {t.column})")
                lines.append(f"â”‚")
                lines.append(f"â”‚ Type: {t.type.value}")
                lines.append(f"â”‚ Impact: {impact_icon} {t.performance_impact.value.upper()}")
                lines.append(f"â”‚")
                lines.append(f"â”‚ {t.explanation}")
                lines.append(f"â””{'â”€' * 65}")
                lines.append("")

        # Warnings
        if analysis.warnings:
            lines.append("PERFORMANCE WARNINGS")
            lines.append("-" * 70)

            for w in analysis.warnings:
                lines.append(f"âš ï¸  {w.message}")
                lines.append(f"   ðŸ’¡ {w.suggestion}")
                lines.append("")

        # Anti-Pattern Detection - ALWAYS INCLUDE THIS SECTION
        lines.append("ANTI-PATTERN DETECTION & RESEARCH")
        lines.append("=" * 70)

        anti_patterns = self.analyzer.detect_dax_anti_patterns(dax_expression)

        if anti_patterns.get('success'):
            patterns_detected = anti_patterns.get('patterns_detected', 0)

            if patterns_detected > 0:
                lines.append(f"âœ… Analysis complete: Detected {patterns_detected} anti-pattern(s)")
                lines.append("")

                # List matched patterns
                for pattern_id, matches in anti_patterns.get('pattern_matches', {}).items():
                    article = next((a for a in anti_patterns.get('articles', []) if a['id'] == pattern_id), None)
                    if article:
                        lines.append(f"âš ï¸  {article['title']}")
                        lines.append(f"   Pattern: {pattern_id}")
                        lines.append(f"   Occurrences: {len(matches)}")

                        # Show research article content (first few lines as summary)
                        if article.get('content'):
                            import textwrap
                            lines.append(f"   Research:")
                            lines.append("")
                            # Dedent and clean up content
                            content = textwrap.dedent(article['content']).strip()
                            content_lines = [line for line in content.split('\n') if line.strip()]

                            # Show first 5 lines as summary
                            for cline in content_lines[:5]:
                                lines.append(f"      {cline.strip()}")

                            if len(content_lines) > 5:
                                lines.append(f"      ...")
                            lines.append("")

                        # Show URL if available
                        if article.get('url'):
                            lines.append(f"   ðŸ“š Source: {article['url']}")

                        lines.append("")

                # List recommendations
                if anti_patterns.get('recommendations'):
                    lines.append("ðŸ“‹ RECOMMENDATIONS:")
                    for rec in anti_patterns['recommendations']:
                        lines.append(f"   â€¢ {rec}")
                    lines.append("")
            else:
                lines.append("âœ… No anti-patterns detected - code follows DAX best practices!")
                lines.append("")

                # Still show that research was performed
                if anti_patterns.get('articles'):
                    lines.append(f"â„¹ï¸  Analyzed against {len(anti_patterns['articles'])} research patterns")
                    lines.append("")
        else:
            # Analysis failed - show error
            error_msg = anti_patterns.get('error', 'Unknown error')
            lines.append(f"âš ï¸  Anti-pattern detection could not be completed: {error_msg}")
            lines.append("   ðŸ’¡ This is usually due to missing research dependencies.")
            lines.append("")

        # Generate specific improvements with new DAX code
        improvements = None
        vertipaq_analysis_result = None

        # Get VertiPaq analysis if connection available
        if connection_state:
            try:
                from core.dax.vertipaq_analyzer import VertiPaqAnalyzer
                vertipaq = VertiPaqAnalyzer(connection_state)
                vertipaq_analysis_result = vertipaq.analyze_dax_columns(dax_expression)
            except:
                pass

        if include_optimization:
            improvements = self.generate_improved_dax(
                dax_expression=dax_expression,
                context_analysis=analysis,
                anti_patterns=anti_patterns,
                vertipaq_analysis=vertipaq_analysis_result
            )

            if improvements.get('has_improvements'):
                lines.append("SPECIFIC IMPROVEMENTS & NEW DAX CODE")
                lines.append("=" * 70)
                lines.append("")
                lines.append(improvements['summary'])
                lines.append("")

                for i, improvement in enumerate(improvements['improvements'], 1):
                    severity_icon = "ðŸ”´" if improvement.get('severity') == 'high' else "ðŸŸ¡" if improvement.get('severity') == 'medium' else "ðŸ”µ"
                    lines.append(f"{severity_icon} IMPROVEMENT {i}: {improvement['issue']}")
                    lines.append("-" * 70)
                    lines.append(f"Explanation: {improvement['explanation']}")
                    lines.append("")

                    if improvement.get('specific_suggestion'):
                        lines.append(f"ðŸ’¡ Specific Action: {improvement['specific_suggestion']}")
                        lines.append("")

                    # Show original pattern with DAX formatting
                    if improvement.get('original_pattern'):
                        lines.append("âŒ ORIGINAL PATTERN:")
                        lines.append("")
                        lines.append("```dax")
                        lines.append(improvement['original_pattern'])
                        lines.append("```")
                        lines.append("")

                    # Show improved pattern with DAX formatting
                    if improvement.get('improved_pattern'):
                        lines.append("âœ… IMPROVED PATTERN:")
                        lines.append("")
                        lines.append("```dax")
                        lines.append(improvement['improved_pattern'])
                        lines.append("```")
                        lines.append("")

                    # Show full improved code if available with DAX formatting
                    if improvement.get('improved_code'):
                        lines.append("âœ… SUGGESTED REFACTORED CODE:")
                        lines.append("")
                        lines.append("```dax")
                        lines.append(improvement['improved_code'])
                        lines.append("```")
                        lines.append("")

                    lines.append("")

        # Traditional Optimizations (generic patterns)
        if include_optimization:
            optimizations = self.suggest_optimizations(analysis)

            if optimizations:
                lines.append("GENERAL OPTIMIZATION PATTERNS")
                lines.append("-" * 70)

                for opt in optimizations:
                    icon = "ðŸ”´" if opt.severity == "critical" else "ðŸŸ¡" if opt.severity == "warning" else "ðŸ”µ"
                    lines.append(f"{icon} [{opt.category.upper()}] {opt.message}")
                    lines.append(f"   ðŸ’¡ {opt.suggestion}")

                    if opt.code_example:
                        lines.append("   Example:")
                        for line in opt.code_example.split("\n"):
                            lines.append(f"      {line}")

                    lines.append("")

        # ===== NEW ENHANCEMENTS =====

        # VertiPaq Column Analysis
        if connection_state:
            try:
                from core.dax.vertipaq_analyzer import VertiPaqAnalyzer

                vertipaq = VertiPaqAnalyzer(connection_state)
                column_analysis = vertipaq.analyze_dax_columns(dax_expression)

                if column_analysis.get('success') and column_analysis.get('columns_analyzed', 0) > 0:
                    lines.append("VERTIPAQ COLUMN ANALYSIS")
                    lines.append("=" * 70)

                    # Show columns referenced with their metrics
                    col_details = column_analysis.get('column_analysis', {})
                    lines.append(f"Columns Referenced: {column_analysis['columns_analyzed']}")
                    lines.append("")

                    # Track if any columns were calculated vs DMV
                    has_calculated = False

                    # Show all columns with their cardinality
                    for col_ref, col_data in col_details.items():
                        if isinstance(col_data, dict) and 'cardinality' in col_data:
                            cardinality = col_data.get('cardinality', 0)
                            encoding = col_data.get('encoding', 'unknown')

                            if encoding == 'calculated':
                                lines.append(f"   â€¢ {col_ref} - Cardinality: {cardinality:,} (calculated)")
                                has_calculated = True
                            else:
                                lines.append(f"   â€¢ {col_ref} - Cardinality: {cardinality:,}")
                        else:
                            # Column without metrics
                            lines.append(f"   â€¢ {col_ref} - No metrics available")

                    lines.append("")

                    # Show totals
                    total_cardinality = column_analysis.get('total_cardinality', 0)
                    total_size_mb = column_analysis.get('total_size_mb', 0)

                    lines.append(f"Total Cardinality: {total_cardinality:,}")

                    # Note about size calculation
                    if has_calculated:
                        lines.append(f"Total Size: {total_size_mb:.2f} MB (estimated for calculated columns)")
                    else:
                        lines.append(f"Total Size: {total_size_mb:.2f} MB")

                    lines.append("")

                    if column_analysis.get('high_cardinality_columns'):
                        lines.append("âš ï¸  HIGH-CARDINALITY COLUMNS:")
                        for col in column_analysis['high_cardinality_columns']:
                            col_data = column_analysis['column_analysis'].get(col, {})
                            lines.append(f"   â€¢ {col}")
                            lines.append(f"     Cardinality: {col_data.get('cardinality', 0):,}")
                            lines.append(f"     Size: {col_data.get('size_mb', 0):.2f} MB")
                            lines.append(f"     Impact: {col_data.get('performance_impact', 'unknown').upper()}")
                            lines.append(f"     ðŸ’¡ {col_data.get('recommendation', '')}")
                            lines.append("")

                    if column_analysis.get('optimizations'):
                        lines.append("DATA TYPE OPTIMIZATIONS:")
                        for opt in column_analysis['optimizations']:
                            lines.append(f"   {opt['severity'].upper()}: {opt['issue']}")
                            lines.append(f"   ðŸ’¡ {opt['recommendation']}")
                            lines.append("")

                    lines.append("")

            except Exception as e:
                logger.warning(f"VertiPaq analysis failed: {e}", exc_info=True)
                lines.append("VERTIPAQ COLUMN ANALYSIS")
                lines.append("=" * 70)
                lines.append(f"âš ï¸  VertiPaq analysis could not be completed: {str(e)}")
                lines.append("")

        # Call Tree Visualization
        try:
            from core.dax.call_tree_builder import CallTreeBuilder

            call_tree_builder = CallTreeBuilder()
            if connection_state:
                try:
                    from core.dax.vertipaq_analyzer import VertiPaqAnalyzer
                    vertipaq = VertiPaqAnalyzer(connection_state)
                    call_tree_builder.vertipaq_analyzer = vertipaq
                except:
                    pass

            call_tree = call_tree_builder.build_call_tree(dax_expression)

            lines.append("CALL TREE HIERARCHY")
            lines.append("=" * 70)
            lines.append("")
            lines.append("```")
            tree_viz = call_tree_builder.visualize_tree(call_tree)
            lines.append(tree_viz)
            lines.append("```")
            lines.append("")

            # Show iteration estimates
            def count_iterations(node):
                total = node.estimated_iterations or 0
                for child in node.children:
                    total += count_iterations(child)
                return total

            total_iterations = count_iterations(call_tree)
            if total_iterations > 0:
                lines.append(f"ðŸ“Š ESTIMATED ITERATIONS: {total_iterations:,}")
                if total_iterations >= 1_000_000:
                    lines.append("   ðŸ”´ CRITICAL: Over 1 million iterations!")
                    lines.append("   This will cause severe performance issues.")
                elif total_iterations >= 100_000:
                    lines.append("   âš ï¸  WARNING: Over 100,000 iterations")
                    lines.append("   Consider optimization.")
                lines.append("")

        except Exception as e:
            logger.warning(f"Call tree analysis failed: {e}", exc_info=True)
            lines.append("CALL TREE HIERARCHY")
            lines.append("=" * 70)
            lines.append(f"âš ï¸  Call tree analysis could not be completed: {str(e)}")
            lines.append("   ðŸ’¡ This usually means measure dependencies couldn't be resolved.")
            lines.append("")

        # Calculation Group Analysis
        if connection_state:
            try:
                from core.dax.calculation_group_analyzer import CalculationGroupAnalyzer

                calc_group_analyzer = CalculationGroupAnalyzer(connection_state)
                calc_group_analysis = calc_group_analyzer.analyze_dax_with_calc_groups(dax_expression)

                if calc_group_analysis.get('success') and calc_group_analysis.get('group_count', 0) > 0:
                    lines.append("CALCULATION GROUP ANALYSIS")
                    lines.append("=" * 70)
                    lines.append(f"Calculation groups detected: {calc_group_analysis['group_count']}")
                    lines.append(f"Groups: {', '.join(calc_group_analysis['groups_detected'])}")
                    lines.append(f"Performance impact: {calc_group_analysis['performance_impact'].upper()}")
                    lines.append("")

                    if calc_group_analysis.get('precedence_conflicts'):
                        lines.append("âš ï¸  PRECEDENCE CONFLICTS:")
                        for conflict in calc_group_analysis['precedence_conflicts']:
                            lines.append(f"   {conflict['severity'].upper()}: {conflict['warning']}")
                            group_list = ', '.join([g['name'] + ' (prec: ' + str(g['precedence']) + ')' for g in conflict['groups']])
                            lines.append(f"   Groups: {group_list}")
                            lines.append("")

                    if calc_group_analysis.get('issues'):
                        lines.append("CALCULATION GROUP ISSUES:")
                        for issue in calc_group_analysis['issues']:
                            lines.append(f"   {issue['severity'].upper()}: {issue['message']}")
                            lines.append(f"   ðŸ’¡ {issue['recommendation']}")
                            lines.append("")

                    if calc_group_analysis.get('recommendations'):
                        lines.append("RECOMMENDATIONS:")
                        for rec in calc_group_analysis['recommendations']:
                            lines.append(f"   â€¢ {rec}")
                        lines.append("")

            except Exception as e:
                logger.warning(f"Calculation group analysis failed: {e}", exc_info=True)
                lines.append("CALCULATION GROUP ANALYSIS")
                lines.append("=" * 70)
                lines.append(f"âš ï¸  Calculation group analysis could not be completed: {str(e)}")
                lines.append("")

        # SUMMARIZE Pattern Detection
        summarize_analysis = self.analyzer.detect_summarize_patterns(dax_expression)
        if summarize_analysis.get('has_summarize'):
            lines.append("SUMMARIZE vs SUMMARIZECOLUMNS")
            lines.append("=" * 70)
            lines.append(f"âš ï¸  {summarize_analysis['recommendation']}")
            lines.append("")
            lines.append("Example conversion:")
            lines.append(summarize_analysis['example_conversion'])
            lines.append("")

        # Variable Optimization Scanner
        try:
            from core.dax.code_rewriter import VariableOptimizationScanner

            var_scanner = VariableOptimizationScanner()
            var_analysis = var_scanner.scan_for_optimizations(dax_expression)

            if var_analysis.get('success') and var_analysis.get('opportunities_found', 0) > 0:
                lines.append("VARIABLE OPTIMIZATION OPPORTUNITIES")
                lines.append("=" * 70)
                lines.append(var_analysis['recommendation'])
                lines.append("")

                lines.append(f"Total potential savings: {var_analysis['total_potential_savings_percent']}%")
                lines.append("")

                for opp in var_analysis['opportunities'][:5]:  # Show top 5
                    lines.append(f"   {opp['priority'].upper()}: {opp['type']}")
                    lines.append(f"   {opp['suggestion']}")
                    if opp.get('example_code'):
                        lines.append(f"   Example:\n   {opp['example_code']}")
                    lines.append("")

        except Exception as e:
            logger.warning(f"Variable optimization scan failed: {e}", exc_info=True)
            lines.append("VARIABLE OPTIMIZATION OPPORTUNITIES")
            lines.append("=" * 70)
            lines.append(f"âš ï¸  Variable optimization scan could not be completed: {str(e)}")
            lines.append("")

        # Code Rewriting Suggestions
        if include_optimization:
            try:
                from core.dax.code_rewriter import DaxCodeRewriter

                rewriter = DaxCodeRewriter()
                rewrite_result = rewriter.rewrite_dax(dax_expression)

                if rewrite_result.get('has_changes'):
                    lines.append("CODE REWRITING SUGGESTIONS")
                    lines.append("=" * 70)
                    lines.append(f"Transformations applied: {rewrite_result['transformation_count']}")
                    lines.append("")

                    for trans in rewrite_result['transformations']:
                        lines.append(f"   {trans['type'].upper()}:")
                        lines.append(f"   {trans['explanation']}")
                        lines.append(f"   Estimated improvement: {trans['estimated_improvement']}")
                        lines.append(f"   Confidence: {trans['confidence']}")
                        lines.append("")

                    if rewrite_result.get('rewritten_code'):
                        lines.append("   REWRITTEN CODE:")
                        for line in rewrite_result['rewritten_code'].split('\n')[:20]:
                            lines.append(f"   {line}")
                        lines.append("")

            except Exception as e:
                logger.warning(f"Code rewriting failed: {e}", exc_info=True)
                lines.append("CODE REWRITING SUGGESTIONS")
                lines.append("=" * 70)
                lines.append(f"âš ï¸  Code rewriting could not be completed: {str(e)}")
                lines.append("")

        # Visual Context Flow Diagram
        try:
            from core.dax.visual_flow import VisualFlowDiagramGenerator

            flow_gen = VisualFlowDiagramGenerator()
            ascii_diagram = flow_gen.generate_ascii_diagram(analysis, dax_expression)

            lines.append("")
            lines.append("```")
            lines.append(ascii_diagram)
            lines.append("```")
            lines.append("")

        except Exception as e:
            logger.warning(f"Visual flow diagram failed: {e}", exc_info=True)
            lines.append("")
            lines.append("VISUAL CONTEXT FLOW DIAGRAM")
            lines.append("=" * 70)
            lines.append(f"âš ï¸  Visual flow diagram could not be generated: {str(e)}")
            lines.append("")

        # ===== END NEW ENHANCEMENTS =====

        # ===== FINAL OPTIMIZED MEASURE - THE COMPLETE RESULT =====
        lines.append("")
        lines.append("=" * 70)
        lines.append("ðŸŽ¯ FINAL OPTIMIZED MEASURE - READY TO USE")
        lines.append("=" * 70)
        lines.append("")

        # Check if we have any improvements at all
        if improvements and improvements.get('has_improvements'):
            # Calculate estimated improvement
            high_priority = sum(1 for i in improvements.get('improvements', []) if i.get('severity') == 'high')
            medium_priority = sum(1 for i in improvements.get('improvements', []) if i.get('severity') == 'medium')
            transformation_count = improvements.get('improvements_count', 0)

            # Case 1: We have automatically transformed code
            if improvements.get('suggested_code'):
                lines.append("âœ… OPTIMIZED DAX CODE (AUTOMATIC TRANSFORMATION):")
                lines.append("")

                # Show the complete optimized code with syntax highlighting
                optimized_code = improvements['suggested_code']
                lines.append("```dax")
                lines.append(optimized_code)
                lines.append("```")
                lines.append("")

                # Show what was optimized
                lines.append("ðŸ“‹ APPLIED OPTIMIZATIONS:")
                lines.append("")
                lines.append(f"   Total transformations applied: {transformation_count}")
                lines.append("")

                # List all transformations
                if improvements.get('transformation_details'):
                    for idx, detail in enumerate(improvements['transformation_details'], 1):
                        lines.append(f"   {idx}. {detail}")
                    lines.append("")

                # Show key metrics
                lines.append("ðŸ“Š OPTIMIZATION SUMMARY:")
                lines.append("")

                if high_priority > 0:
                    lines.append(f"   â€¢ {high_priority} high-priority optimization(s) applied")
                if medium_priority > 0:
                    lines.append(f"   â€¢ {medium_priority} medium-priority optimization(s) applied")

                # Add performance estimate
                estimated_gain = min(high_priority * 20 + medium_priority * 10, 80)
                if estimated_gain > 0:
                    lines.append(f"   â€¢ Estimated performance gain: {estimated_gain}%")

                lines.append("")

                # Show comparison with proper DAX formatting
                lines.append("ðŸ“ˆ BEFORE vs AFTER COMPARISON:")
                lines.append("")
                lines.append("ORIGINAL CODE:")
                lines.append("")
                lines.append("```dax")
                lines.append(dax_expression.strip())
                lines.append("```")
                lines.append("")

                lines.append("OPTIMIZED CODE:")
                lines.append("")
                lines.append("```dax")
                lines.append(optimized_code.strip())
                lines.append("```")
                lines.append("")

                lines.append("ðŸ’¡ NEXT STEPS:")
                lines.append("")
                lines.append("   1. Copy the optimized code above")
                lines.append("   2. Test it in your Power BI model")
                lines.append("   3. Verify the results match the original")
                lines.append("   4. Use DAX Studio to compare performance")
                lines.append("   5. Update your measure with the optimized version")
                lines.append("")

            # Case 2: We have improvements but no automatic transformation
            else:
                lines.append("âš ï¸  OPTIMIZATION OPPORTUNITIES DETECTED")
                lines.append("-" * 70)
                lines.append("")
                lines.append(f"Found {transformation_count} improvement(s) that require manual optimization:")
                lines.append("")

                # Show key metrics
                lines.append("ðŸ“Š IMPROVEMENT BREAKDOWN:")
                lines.append("")
                if high_priority > 0:
                    lines.append(f"   ðŸ”´ {high_priority} high-priority issue(s) - SHOULD BE ADDRESSED")
                if medium_priority > 0:
                    lines.append(f"   ðŸŸ¡ {medium_priority} medium-priority improvement(s) - RECOMMENDED")

                # Add performance estimate
                estimated_gain = min(high_priority * 20 + medium_priority * 10, 80)
                if estimated_gain > 0:
                    lines.append(f"   ðŸ“ˆ Estimated performance gain: up to {estimated_gain}%")

                lines.append("")
                lines.append("âš ï¸  IMPORTANT: Review the 'SPECIFIC IMPROVEMENTS & NEW DAX CODE' section above")
                lines.append("    for detailed recommendations and code patterns to apply.")
                lines.append("")

                lines.append("CURRENT CODE:")
                lines.append("")
                lines.append("```dax")
                lines.append(dax_expression.strip())
                lines.append("```")
                lines.append("")

                lines.append("ðŸ’¡ NEXT STEPS:")
                lines.append("")
                lines.append("   1. Review the improvement recommendations above")
                lines.append("   2. Apply the suggested patterns to your DAX code")
                lines.append("   3. Test each change incrementally")
                lines.append("   4. Use DAX Studio to measure performance improvements")
                lines.append("   5. Consider using AI assistance to generate optimized code")
                lines.append("")

        else:
            # No improvements - code is already optimal
            lines.append("âœ… YOUR CODE IS ALREADY WELL-OPTIMIZED!")
            lines.append("-" * 70)
            lines.append("")
            lines.append("No significant optimizations were identified.")
            lines.append("Your DAX code follows best practices.")
            lines.append("")
            lines.append("CURRENT CODE:")
            lines.append("")
            lines.append("```dax")
            lines.append(dax_expression.strip())
            lines.append("```")
            lines.append("")
            lines.append("ðŸ’¡ RECOMMENDATIONS:")
            lines.append("")
            lines.append("   â€¢ Monitor query performance in DAX Studio")
            lines.append("   â€¢ Check VertiPaq Analyzer for column-level optimizations")
            lines.append("   â€¢ Review data model relationships and cardinality")
            lines.append("   â€¢ Consider adding measures to display folders for organization")
            lines.append("")

        lines.append("=" * 70)
        lines.append("END OF ENHANCED ANALYSIS REPORT")
        lines.append("=" * 70)

        return "\n".join(lines)
