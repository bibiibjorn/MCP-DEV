# MCP Server Refactoring Plan
## Comprehensive Plan for Structure, Performance, and Extensibility Improvements

**Version:** 1.0
**Date:** 2025-11-19
**Current Server Version:** v6.01

---

## Executive Summary

This refactoring plan outlines a comprehensive strategy to enhance the MCP-PowerBI-Finvision server for improved structure, performance, and extensibility. The plan is organized into **4 phases** spanning approximately **8-12 weeks**, focusing on critical architectural improvements while maintaining backward compatibility.

### Key Improvements

1. **Activate Dormant Intelligence Features** - Integrate existing but unused middleware and smart routing
2. **Dynamic Tool Prioritization System** - Context-aware tool ordering based on usage patterns
3. **Architectural Cleanup** - Eliminate global state, improve testability, and reduce coupling
4. **Performance Optimizations** - Reduce latency by 20-30% through caching and async improvements
5. **Enhanced Extensibility** - Plugin architecture for easy feature additions

### Expected Outcomes

- **Performance**: 20-30% reduction in average response time
- **Maintainability**: 60%+ test coverage, reduced coupling, clearer ownership
- **Extensibility**: Add features like tool prioritization in hours instead of days
- **Developer Experience**: Better debugging, easier testing, clearer architecture

---

## Table of Contents

1. [Current Architecture Analysis](#1-current-architecture-analysis)
2. [Identified Issues & Opportunities](#2-identified-issues--opportunities)
3. [Refactoring Phases](#3-refactoring-phases)
4. [Detailed Implementation Plans](#4-detailed-implementation-plans)
5. [Performance Impact Analysis](#5-performance-impact-analysis)
6. [Risk Assessment](#6-risk-assessment)
7. [Testing Strategy](#7-testing-strategy)
8. [Migration & Rollout](#8-migration--rollout)
9. [Success Metrics](#9-success-metrics)

---

## 1. Current Architecture Analysis

### 1.1 Architecture Strengths

âœ… **Modular Design**: Clear separation between handlers, core logic, and infrastructure
âœ… **Registry Pattern**: Elegant tool discovery and management
âœ… **Rich Error Handling**: Comprehensive error responses with recovery guidance
âœ… **Flexible Configuration**: Two-tier config with feature flags
âœ… **Multiple Optimization Layers**: Caching, fast paths, lazy initialization
âœ… **Intelligence Infrastructure**: Context tracking and smart routing capabilities exist

### 1.2 Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Server (pbixray_server_enhanced)  â”‚
â”‚   - list_tools() â†’ Registry             â”‚
â”‚   - call_tool() â†’ Dispatcher            â”‚
â”‚   - list/read_resource() â†’ Resources    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                      â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Registry â”‚          â”‚Dispatcherâ”‚
   â”‚  System  â”‚          â”‚  System  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                      â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   26 Handler Modules         â”‚
   â”‚   (connection, metadata,     â”‚
   â”‚    query, analysis, etc.)    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Core Business Logic        â”‚
   â”‚   (14+ manager classes)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Key Metrics

- **Total Lines of Code**: ~66,445 lines
- **Handler Modules**: 26 handlers
- **Registered Tools**: 60+ tools across 13 categories
- **Core Managers**: 14+ specialized managers
- **Test Coverage**: ~150 lines (3 test files) - **needs improvement**

### 1.4 Technology Stack

- **Server Framework**: MCP (Model Context Protocol) via stdio
- **Async Runtime**: asyncio
- **Language**: Python 3.8+
- **Key Dependencies**: AMO (Analysis Management Objects), pandas, python-docx

---

## 2. Identified Issues & Opportunities

### 2.1 Critical Issues

#### ðŸ”´ **Issue #1: Dormant Intelligence Features**
**Location**: `server/intelligent_middleware.py`, `core/intelligence/tool_router.py`
**Problem**: Sophisticated middleware and NLP routing exist but are NOT integrated into main execution flow
**Impact**: Users miss out on:
- Context-aware suggestions
- Smart workflow recommendations
- Dynamic parameter defaults
- Tool relationship awareness

**Current State**:
```python
# pbixray_server_enhanced.py line 178
result = dispatcher.dispatch(name, arguments)  # No middleware integration
```

**Evidence**:
- `IntelligentMiddleware` has `pre_process_request()` and `post_process_result()` methods (266 lines)
- `IntelligentToolRouter` has comprehensive intent matching (492 lines)
- Neither is called in `call_tool()` handler

---

#### ðŸ”´ **Issue #2: Global State Management**
**Location**: `core/infrastructure/connection_state.py`
**Problem**: Single global `connection_state` singleton with 20+ manager attributes
**Impact**:
- Hard to test (requires global state setup)
- Not truly thread-safe at application level
- Unclear ownership and lifecycle
- Difficult to mock for unit tests

**Current State**:
```python
# 20+ global state attributes
class ConnectionState:
    def __init__(self):
        self.connection_manager = None
        self.query_executor = None
        self.performance_analyzer = None
        self.dax_injector = None
        self.bpa_analyzer = None
        # ... 15 more managers
```

**Evidence**: 548 lines of state management with threading locks and complex initialization

---

#### ðŸŸ¡ **Issue #3: Static Tool Prioritization**
**Location**: `server/registry.py`
**Problem**: Tools ordered by static `sort_order` attribute only
**Impact**:
- No context-aware prioritization
- Can't learn from usage patterns
- Frequently-used tools buried in lists
- No way to boost tools based on current context

**Current State**:
```python
@dataclass
class ToolDefinition:
    sort_order: int = 999  # Static only
```

**Opportunity**: Your example of wanting to prioritize tools based on request type is currently **hard** because:
- No dynamic scoring system
- No usage tracking for frequency
- No context relevance calculation
- Would require modifying `get_all_tools_as_mcp()` and adding complex logic

---

#### ðŸŸ¡ **Issue #4: Tool Consolidation Incomplete**
**Location**: `server/dispatch.py` (60+ tool mappings)
**Problem**: Legacy individual tools coexist with new consolidated operations
**Impact**:
- Confusing for users (two ways to do same thing)
- More maintenance burden
- Inconsistent patterns

**Evidence**:
```python
# Both legacy and consolidated tools registered
'02_table_operations': 'table_operations',  # Preferred
'02_list_tables': 'list_tables',           # Legacy (still exists)
'02_describe_table': 'describe_table',     # Legacy (still exists)
```

---

#### ðŸŸ¡ **Issue #5: Insufficient Test Coverage**
**Location**: `tests/` (3 files, ~150 lines)
**Problem**: Minimal testing for 66K+ lines of code
**Impact**:
- High regression risk during refactoring
- Unclear if changes break existing functionality
- Hard to verify edge cases

**Evidence**:
- Only 3 test files covering specific handlers
- No integration tests
- No tests for caching, middleware, routing
- No performance benchmarks

---

#### ðŸŸ¡ **Issue #6: Manager Lifecycle Management**
**Location**: Throughout `connection_state.py` and handlers
**Problem**: Lazy initialization with complex locking, unclear ownership
**Impact**:
- Hard to trace when managers are created
- Difficult to mock for testing
- Unclear dependency relationships

**Current Pattern**:
```python
# Lazy init with double-check locking (repeated 14+ times)
if self.query_executor is None:
    with self._init_lock:
        if self.query_executor is None:
            self.query_executor = QueryExecutor(...)
```

---

#### ðŸŸ¢ **Issue #7: Cache Coordination**
**Location**: Multiple locations (connection_state, cache_manager, query executor)
**Problem**: Multiple independent caches with no unified invalidation
**Impact**:
- Cache inconsistency risk
- No coordinated cache warming
- Hard to clear all caches at once

**Evidence**:
- Table mapping cache in `connection_state`
- Query result cache in query executor
- General cache in `EnhancedCacheManager`
- Each has own TTL and invalidation logic

---

#### ðŸŸ¢ **Issue #8: Type Safety**
**Location**: Throughout codebase
**Problem**: Mostly `Dict[str, Any]` returns, no compile-time type checking
**Impact**:
- Runtime errors that could be caught at dev time
- Hard to understand expected response structure
- IDE autocomplete limited

**Example**:
```python
def dispatch(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
    # What fields does the dict have? Unknown without reading code
```

---

### 2.2 Opportunities for Enhancement

#### ðŸ’¡ **Opportunity #1: Dynamic Tool Prioritization**
**Ease**: EASY to implement (2-3 days) after middleware activation
**Value**: HIGH - directly addresses your use case

**Implementation Approach**:
```python
class ToolPriorityManager:
    def calculate_priority(self, tool_name: str, context: Dict[str, Any]) -> float:
        """Calculate dynamic priority score (0-100)"""
        base = registry.get_tool_def(tool_name).sort_order

        # Usage frequency boost (0-20 points)
        usage_boost = self.get_usage_frequency(tool_name) * 20

        # Context relevance boost (0-30 points)
        context_boost = self.get_context_relevance(tool_name, context) * 30

        # Connection state boost (0-10 points)
        connection_boost = 10 if self.is_tool_available(tool_name) else 0

        return base + usage_boost + context_boost + connection_boost
```

**Benefits**:
- Tools auto-prioritize based on what user does most
- Connected vs disconnected tools properly ordered
- Context-aware (if analyzing measures, measure tools ranked higher)
- Learn from usage patterns over time

---

#### ðŸ’¡ **Opportunity #2: Plugin Architecture**
**Ease**: MODERATE (1-2 weeks)
**Value**: HIGH - makes adding features like tool prioritization trivial

**Concept**:
```python
class ToolPlugin(ABC):
    @abstractmethod
    def on_tool_registered(self, tool_def: ToolDefinition) -> None: pass

    @abstractmethod
    def on_tool_call(self, name: str, args: Dict) -> Optional[Dict]: pass

    @abstractmethod
    def on_tool_result(self, name: str, result: Dict) -> Dict: pass

# Example plugin for prioritization
class PrioritizationPlugin(ToolPlugin):
    def on_tool_call(self, name: str, args: Dict):
        self.usage_tracker.record_call(name)
        return None  # Don't intercept

    def on_tool_result(self, name: str, result: Dict):
        result['_priority_score'] = self.calculate_priority(name)
        return result
```

**Benefits**:
- Add features without modifying core
- Enable/disable features via config
- Community plugins possible
- A/B test new features easily

---

#### ðŸ’¡ **Opportunity #3: Response Streaming**
**Ease**: MODERATE (1 week)
**Value**: MEDIUM - better UX for long-running operations

**Concept**: Stream partial results for tools like `full_analysis` and `export_tmsl`
- Show progress during execution
- User sees results incrementally
- Can cancel long operations

---

#### ðŸ’¡ **Opportunity #4: Declarative Tool Definitions**
**Ease**: EASY (3-5 days)
**Value**: MEDIUM - easier tool creation

**Current**: Tools defined in Python code across 26 handler files
**Proposed**: YAML/JSON tool definitions with code generation

```yaml
# tools/metadata/list_tables.yaml
name: list_tables
category: metadata
sort_order: 10
description: List all tables in the model
requires_connection: true
handler: core.metadata.list_tables
input_schema:
  type: object
  properties:
    summary_only:
      type: boolean
      default: false
```

**Benefits**:
- Tools defined declaratively
- Auto-generate schemas and docs
- Validate definitions at build time
- Non-Python contributors can add tools

---

## 3. Refactoring Phases

### Phase Overview

| Phase | Duration | Priority | Complexity | Risk |
|-------|----------|----------|------------|------|
| **Phase 1**: Activate Intelligence | 2 weeks | ðŸ”´ Critical | Low | Low |
| **Phase 2**: Tool Prioritization & Cleanup | 2 weeks | ðŸŸ¡ High | Medium | Low |
| **Phase 3**: Architectural Refactoring | 3 weeks | ðŸŸ¡ High | High | Medium |
| **Phase 4**: Testing & Performance | 2-3 weeks | ðŸŸ¢ Medium | Medium | Low |

**Total Estimated Duration**: 8-12 weeks (can run some phases in parallel)

---

### Phase 1: Activate Intelligence Features (Weeks 1-2)
**Goal**: Enable existing but dormant intelligence features
**Risk**: Low (features already built, just need integration)
**Impact**: High (immediate user experience improvement)

**Tasks**:
1. âœ… Integrate `IntelligentMiddleware` into `call_tool()` flow
2. âœ… Enable pre/post-processing hooks
3. âœ… Activate context tracking
4. âœ… Enable suggestion engine
5. âœ… Add intelligent routing to dispatcher
6. âœ… Add feature flag for gradual rollout

**Deliverables**:
- Users get automatic suggestions after each tool call
- Smart defaults applied based on context
- Workflow recommendations when using multiple tools
- Related tools shown in responses

---

### Phase 2: Tool Prioritization & Cleanup (Weeks 3-4)
**Goal**: Implement dynamic tool prioritization and complete tool consolidation
**Risk**: Low (backward compatible changes)
**Impact**: High (directly addresses your use case)

**Tasks**:
1. âœ… Implement `ToolPriorityManager` with usage tracking
2. âœ… Add context-aware priority scoring
3. âœ… Integrate priority manager into `get_all_tools_as_mcp()`
4. âœ… Complete tool consolidation (deprecate legacy tools)
5. âœ… Add tool usage analytics dashboard
6. âœ… Implement priority learning from user patterns

**Deliverables**:
- Tools automatically prioritized based on usage and context
- Single set of consolidated tools (legacy deprecated)
- Usage analytics showing most-used tools
- Priority scores visible in tool metadata

---

### Phase 3: Architectural Refactoring (Weeks 5-7)
**Goal**: Eliminate global state, improve testability, add plugin system
**Risk**: Medium (larger architectural changes)
**Impact**: High (better maintainability and extensibility)

**Tasks**:
1. âœ… Replace global `connection_state` with dependency injection
2. âœ… Implement `ServiceContainer` for manager lifecycle
3. âœ… Add plugin architecture
4. âœ… Refactor manager initialization to factory pattern
5. âœ… Implement cache coordinator with unified invalidation
6. âœ… Add Pydantic models for type safety
7. âœ… Migrate to async-first architecture

**Deliverables**:
- No more global state (except registry)
- All managers injected via dependency container
- Plugin system for extending functionality
- Type-safe request/response models
- Unified cache management

---

### Phase 4: Testing & Performance (Weeks 8-10)
**Goal**: Achieve 60%+ test coverage and 20-30% performance improvement
**Risk**: Low (quality improvements)
**Impact**: High (confidence in changes, better performance)

**Tasks**:
1. âœ… Write comprehensive unit tests (target 60% coverage)
2. âœ… Write integration tests for key workflows
3. âœ… Add performance benchmarks
4. âœ… Optimize hot paths identified by profiling
5. âœ… Implement response streaming for long operations
6. âœ… Add load testing framework
7. âœ… Performance regression tests

**Deliverables**:
- 60%+ test coverage
- 20-30% faster average response time
- Performance benchmarks and monitoring
- Load testing suite
- CI/CD pipeline with automated tests

---

## 4. Detailed Implementation Plans

### 4.1 Phase 1: Activate Intelligence Features

#### Task 1.1: Integrate IntelligentMiddleware into call_tool()

**Current Code** (`src/pbixray_server_enhanced.py:133-178`):
```python
@app.call_tool()
async def call_tool(name: str, arguments: Any) -> List[TextContent]:
    # ... validation ...
    result = dispatcher.dispatch(name, arguments)
    # ... return ...
```

**Proposed Changes**:
```python
from server.intelligent_middleware import get_intelligent_middleware

middleware = get_intelligent_middleware()

@app.call_tool()
async def call_tool(name: str, arguments: Any) -> List[TextContent]:
    # Pre-processing with intelligence
    pre_result = middleware.pre_process_request(name, arguments)

    if not pre_result['proceed']:
        return [TextContent(type="text", text=json.dumps(pre_result, indent=2))]

    # Use enhanced arguments from middleware
    enhanced_args = pre_result.get('enhanced_arguments', arguments)

    # Add pre-processing suggestions to context
    suggestions = pre_result.get('suggestions', [])

    # ... existing validation ...

    # Dispatch with enhanced arguments
    result = dispatcher.dispatch(name, enhanced_args)

    # Post-processing with intelligence
    result = middleware.post_process_result(name, enhanced_args, result)

    # Merge suggestions from pre-processing
    if suggestions:
        if '_suggestions' not in result:
            result['_suggestions'] = []
        result['_suggestions'].extend(suggestions)

    # ... existing token limits and return ...
```

**Files to Modify**:
- `src/pbixray_server_enhanced.py` (add middleware integration)
- `core/config/default_config.json` (add feature flag `features.enable_intelligent_middleware: true`)

**Testing**:
- Unit test: middleware pre/post processing called correctly
- Integration test: suggestions appear in responses
- Feature flag test: can disable middleware via config

**Rollout Strategy**:
1. Add feature flag (default: disabled)
2. Enable for internal testing
3. Enable for 10% of requests (A/B test)
4. Enable for all requests after validation

---

#### Task 1.2: Enable Context Tracking

**Changes Required**:
- Initialize context tracker in `connection_state` on first tool call
- Track tool usage sequences
- Store focus objects (table, measure, etc.)
- Track issues found during analysis

**Implementation**:
```python
# In IntelligentMiddleware.pre_process_request()
if connection_state.context_tracker.current_context:
    connection_state.context_tracker.add_tool_used(tool_name)
else:
    # Start new context
    focus_object = self._extract_focus_object(tool_name, arguments)
    focus_type = self._infer_focus_type(tool_name)
    if focus_object and focus_type:
        connection_state.context_tracker.start_analysis(focus_object, focus_type)
```

**Benefits**:
- Context persists across tool calls in same session
- Smart defaults based on previous actions
- Better workflow detection

---

#### Task 1.3: Activate Suggestion Engine

**Location**: `core/intelligence/suggestion_engine.py`
**Current State**: Exists but not called in main flow
**Changes**: Called in `middleware.post_process_result()`

**Example Output**:
```json
{
  "success": true,
  "tables": [...],
  "_suggestions": [
    {
      "type": "next_step",
      "priority": "high",
      "action": "describe_table",
      "reason": "You listed tables - consider profiling one with describe_table",
      "suggested_args": {
        "table_name": "Sales"
      }
    }
  ]
}
```

---

#### Task 1.4: Intelligent Routing Integration

**Changes**:
- Add optional NLP-based routing before dispatch
- If user query (not direct tool call), use `IntelligentToolRouter`
- Suggest better tools if mismatch detected

**Config**:
```json
{
  "features": {
    "enable_intelligent_routing": true,
    "routing_confidence_threshold": 0.7
  }
}
```

**Implementation**:
```python
# In dispatcher or middleware
if is_natural_language_request(name, arguments):
    routing = tool_router.route_request(arguments.get('query'), context)
    if routing['routing_strategy'] == 'workflow':
        # Suggest workflow instead of single tool
        return workflow_suggestion_response(routing)
```

---

### 4.2 Phase 2: Tool Prioritization & Cleanup

#### Task 2.1: Implement ToolPriorityManager

**New File**: `server/tool_priority.py`

```python
from typing import Dict, Any, Optional
from dataclasses import dataclass
import time
import json
from pathlib import Path

@dataclass
class ToolUsageStats:
    tool_name: str
    call_count: int = 0
    last_used: float = 0.0
    success_count: int = 0
    avg_duration_ms: float = 0.0
    context_tags: Dict[str, int] = None  # e.g., {'connected': 10, 'metadata': 5}

    def __post_init__(self):
        if self.context_tags is None:
            self.context_tags = {}

class ToolPriorityManager:
    """Manages dynamic tool prioritization based on usage and context"""

    def __init__(self, stats_file: Optional[Path] = None):
        self.stats_file = stats_file or Path("data/tool_usage_stats.json")
        self.usage_stats: Dict[str, ToolUsageStats] = {}
        self._load_stats()

    def record_call(self, tool_name: str, success: bool, duration_ms: float, context: Dict[str, Any]):
        """Record a tool call for usage tracking"""
        if tool_name not in self.usage_stats:
            self.usage_stats[tool_name] = ToolUsageStats(tool_name=tool_name)

        stats = self.usage_stats[tool_name]
        stats.call_count += 1
        stats.last_used = time.time()
        if success:
            stats.success_count += 1

        # Update rolling average duration
        alpha = 0.1  # Smoothing factor
        stats.avg_duration_ms = (alpha * duration_ms) + ((1 - alpha) * stats.avg_duration_ms)

        # Track context tags
        context_tag = context.get('type', 'general')
        stats.context_tags[context_tag] = stats.context_tags.get(context_tag, 0) + 1

        self._save_stats()

    def calculate_priority(self, tool_name: str, context: Optional[Dict[str, Any]] = None) -> float:
        """
        Calculate dynamic priority score (0-100, higher = more priority)

        Factors:
        - Base sort_order from tool definition (0-30 points)
        - Usage frequency (0-25 points)
        - Context relevance (0-25 points)
        - Recency (0-10 points)
        - Success rate (0-10 points)
        """
        from server.registry import get_registry
        registry = get_registry()

        # Base score from tool definition (normalized to 0-30)
        tool_def = registry.get_tool_def(tool_name)
        base_score = max(0, min(30, (1000 - tool_def.sort_order) / 33))

        # Usage frequency score (0-25)
        usage_score = self._calculate_usage_score(tool_name)

        # Context relevance score (0-25)
        context_score = self._calculate_context_score(tool_name, context)

        # Recency score (0-10)
        recency_score = self._calculate_recency_score(tool_name)

        # Success rate score (0-10)
        success_score = self._calculate_success_score(tool_name)

        total = base_score + usage_score + context_score + recency_score + success_score
        return round(total, 2)

    def _calculate_usage_score(self, tool_name: str) -> float:
        """Score based on usage frequency (0-25)"""
        if tool_name not in self.usage_stats:
            return 0.0

        stats = self.usage_stats[tool_name]
        max_calls = max((s.call_count for s in self.usage_stats.values()), default=1)

        # Normalize to 0-25
        return (stats.call_count / max_calls) * 25

    def _calculate_context_score(self, tool_name: str, context: Optional[Dict[str, Any]]) -> float:
        """Score based on context relevance (0-25)"""
        if not context or tool_name not in self.usage_stats:
            return 5.0  # Neutral score

        stats = self.usage_stats[tool_name]
        context_tag = context.get('type', 'general')

        # If tool was frequently used in this context, boost score
        context_count = stats.context_tags.get(context_tag, 0)
        total_context_uses = sum(stats.context_tags.values())

        if total_context_uses == 0:
            return 5.0

        relevance = context_count / total_context_uses
        return relevance * 25

    def _calculate_recency_score(self, tool_name: str) -> float:
        """Score based on recency (0-10)"""
        if tool_name not in self.usage_stats:
            return 0.0

        stats = self.usage_stats[tool_name]
        if stats.last_used == 0:
            return 0.0

        # Tools used in last hour get full score, decay over 7 days
        age_seconds = time.time() - stats.last_used
        age_hours = age_seconds / 3600

        if age_hours < 1:
            return 10.0
        elif age_hours < 24:
            return 7.0
        elif age_hours < 168:  # 7 days
            return 3.0
        else:
            return 0.0

    def _calculate_success_score(self, tool_name: str) -> float:
        """Score based on success rate (0-10)"""
        if tool_name not in self.usage_stats:
            return 5.0  # Neutral

        stats = self.usage_stats[tool_name]
        if stats.call_count == 0:
            return 5.0

        success_rate = stats.success_count / stats.call_count
        return success_rate * 10

    def get_top_tools(self, n: int = 10, context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Get top N tools by priority score"""
        from server.registry import get_registry
        registry = get_registry()

        tools_with_scores = []
        for tool_def in registry.get_all_tools():
            score = self.calculate_priority(tool_def.name, context)
            tools_with_scores.append({
                'name': tool_def.name,
                'score': score,
                'category': tool_def.category
            })

        # Sort by score descending
        tools_with_scores.sort(key=lambda x: x['score'], reverse=True)
        return tools_with_scores[:n]

    def _load_stats(self):
        """Load usage stats from file"""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, 'r') as f:
                    data = json.load(f)
                    for tool_name, stats_dict in data.items():
                        self.usage_stats[tool_name] = ToolUsageStats(
                            tool_name=tool_name,
                            call_count=stats_dict.get('call_count', 0),
                            last_used=stats_dict.get('last_used', 0.0),
                            success_count=stats_dict.get('success_count', 0),
                            avg_duration_ms=stats_dict.get('avg_duration_ms', 0.0),
                            context_tags=stats_dict.get('context_tags', {})
                        )
            except Exception as e:
                logger.warning(f"Failed to load tool usage stats: {e}")

    def _save_stats(self):
        """Save usage stats to file"""
        try:
            self.stats_file.parent.mkdir(parents=True, exist_ok=True)
            data = {
                tool_name: {
                    'call_count': stats.call_count,
                    'last_used': stats.last_used,
                    'success_count': stats.success_count,
                    'avg_duration_ms': stats.avg_duration_ms,
                    'context_tags': stats.context_tags
                }
                for tool_name, stats in self.usage_stats.items()
            }
            with open(self.stats_file, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logger.warning(f"Failed to save tool usage stats: {e}")

# Global instance
_priority_manager: Optional[ToolPriorityManager] = None

def get_priority_manager() -> ToolPriorityManager:
    global _priority_manager
    if _priority_manager is None:
        _priority_manager = ToolPriorityManager()
    return _priority_manager
```

**Integration**:

1. **Record calls in middleware**:
```python
# In IntelligentMiddleware.post_process_result()
from server.tool_priority import get_priority_manager

priority_mgr = get_priority_manager()
priority_mgr.record_call(
    tool_name=tool_name,
    success=result.get('success', False),
    duration_ms=duration,
    context={'type': self._infer_focus_type(tool_name)}
)
```

2. **Sort tools by priority in registry**:
```python
# In HandlerRegistry.get_all_tools_as_mcp()
from server.tool_priority import get_priority_manager

priority_mgr = get_priority_manager()
context = connection_state.context_tracker.get_relevant_context() if connection_state.context_tracker else None

# Sort by dynamic priority instead of static sort_order
sorted_defs = sorted(
    self._handlers.values(),
    key=lambda x: priority_mgr.calculate_priority(x.name, context),
    reverse=True  # Higher score first
)
```

3. **Add priority scores to tool metadata**:
```python
# Option: Include priority score in tool description
for tool_def in sorted_defs:
    score = priority_mgr.calculate_priority(tool_def.name, context)

    # Add score to description or as metadata
    enhanced_description = f"{tool_def.description} [Priority: {score:.0f}]"

    tools.append(Tool(
        name=mcp_name,
        description=enhanced_description,
        inputSchema=tool_def.input_schema
    ))
```

**Benefits**:
- âœ… **Easy to add** - Your example feature is now trivial to implement
- âœ… **Self-learning** - Adapts to user behavior over time
- âœ… **Context-aware** - Tools prioritized based on current task
- âœ… **Performance** - Priority calculation cached, minimal overhead
- âœ… **Explainable** - Priority scores visible and debuggable

**Testing**:
- Unit tests for priority calculation
- Test that frequently-used tools rank higher
- Test context relevance scoring
- Integration test: verify tool order changes over time

---

#### Task 2.2: Complete Tool Consolidation

**Goal**: Remove legacy individual operation tools, keep only consolidated operations

**Changes**:

1. **Mark legacy tools as deprecated** (backward compatible):
```python
# In registry, add deprecation metadata
@dataclass
class ToolDefinition:
    deprecated: bool = False
    replacement: Optional[str] = None
    deprecation_message: Optional[str] = None
```

2. **Add deprecation warnings**:
```python
# In dispatcher.dispatch()
tool_def = self.registry.get_tool_def(internal_name)
if tool_def.deprecated:
    logger.warning(f"Tool {tool_name} is deprecated, use {tool_def.replacement}")
    result['_deprecation_warning'] = {
        'deprecated': True,
        'replacement': tool_def.replacement,
        'message': tool_def.deprecation_message
    }
```

3. **Migration guide**:
Create `docs/TOOL_MIGRATION.md` mapping old -> new tools

4. **Removal schedule**:
- Week 1-2: Add deprecation warnings
- Week 3-6: Monitor usage, assist migrations
- Week 7+: Remove deprecated tools (major version bump)

---

### 4.3 Phase 3: Architectural Refactoring

#### Task 3.1: Replace Global State with Dependency Injection

**New File**: `core/infrastructure/service_container.py`

```python
from typing import Dict, Any, Optional, Type, TypeVar, Callable
import threading

T = TypeVar('T')

class ServiceContainer:
    """Dependency injection container for managing service lifecycle"""

    def __init__(self):
        self._services: Dict[str, Any] = {}
        self._factories: Dict[str, Callable] = {}
        self._singletons: Dict[str, bool] = {}
        self._lock = threading.RLock()

    def register_singleton(self, service_type: Type[T], factory: Callable[[], T]):
        """Register a singleton service"""
        type_name = service_type.__name__
        with self._lock:
            self._factories[type_name] = factory
            self._singletons[type_name] = True

    def register_transient(self, service_type: Type[T], factory: Callable[[], T]):
        """Register a transient (new instance each time) service"""
        type_name = service_type.__name__
        with self._lock:
            self._factories[type_name] = factory
            self._singletons[type_name] = False

    def get(self, service_type: Type[T]) -> T:
        """Get service instance"""
        type_name = service_type.__name__

        with self._lock:
            # Check if already instantiated singleton
            if type_name in self._services:
                return self._services[type_name]

            # Get factory
            if type_name not in self._factories:
                raise KeyError(f"Service {type_name} not registered")

            factory = self._factories[type_name]
            instance = factory()

            # Cache if singleton
            if self._singletons.get(type_name, False):
                self._services[type_name] = instance

            return instance

    def clear(self):
        """Clear all cached service instances"""
        with self._lock:
            self._services.clear()

# Global container
_container: Optional[ServiceContainer] = None

def get_container() -> ServiceContainer:
    global _container
    if _container is None:
        _container = ServiceContainer()
        _register_core_services(_container)
    return _container

def _register_core_services(container: ServiceContainer):
    """Register all core services"""
    from core.infrastructure.connection_manager import ConnectionManager
    from core.execution.query_executor import QueryExecutor
    # ... import other managers

    # Register singletons
    container.register_singleton(ConnectionManager, lambda: ConnectionManager())
    container.register_singleton(QueryExecutor, lambda: QueryExecutor(
        container.get(ConnectionManager)
    ))
    # ... register other services
```

**Migration Strategy**:
1. **Phase 3.1a** (Week 5): Create `ServiceContainer`, register all services
2. **Phase 3.1b** (Week 5-6): Update handlers to use container instead of global state
3. **Phase 3.1c** (Week 6): Remove global `connection_state` attributes one by one
4. **Phase 3.1d** (Week 7): Final cleanup, remove `connection_state` entirely (keep minimal state)

**Example Handler Migration**:

**Before**:
```python
# In handler
from core.infrastructure.connection_state import connection_state

def list_tables(args):
    if not connection_state.is_connected():
        return error_response()

    query_executor = connection_state.get_or_create_query_executor()
    # ...
```

**After**:
```python
# In handler
from core.infrastructure.service_container import get_container
from core.execution.query_executor import QueryExecutor
from core.infrastructure.connection_manager import ConnectionManager

def list_tables(args):
    container = get_container()
    conn_mgr = container.get(ConnectionManager)

    if not conn_mgr.is_connected():
        return error_response()

    query_executor = container.get(QueryExecutor)
    # ...
```

**Benefits**:
- âœ… No global state (except container itself)
- âœ… Easy to test (mock container)
- âœ… Clear dependencies
- âœ… Proper lifecycle management

---

#### Task 3.2: Implement Plugin Architecture

**New File**: `server/plugin_system.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

class ToolPlugin(ABC):
    """Base class for tool plugins"""

    @property
    @abstractmethod
    def plugin_name(self) -> str:
        """Unique plugin name"""
        pass

    def on_tool_registered(self, tool_def: 'ToolDefinition') -> None:
        """Called when a tool is registered"""
        pass

    def on_tool_call_start(self, name: str, args: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Called before tool execution
        Return dict to intercept and return early, None to proceed
        """
        return None

    def on_tool_call_end(self, name: str, args: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Called after tool execution
        Can modify result before returning to user
        """
        return result

    def on_error(self, name: str, args: Dict[str, Any], error: Exception) -> None:
        """Called when tool execution fails"""
        pass

class PluginManager:
    """Manages plugins for the MCP server"""

    def __init__(self):
        self._plugins: List[ToolPlugin] = []

    def register_plugin(self, plugin: ToolPlugin):
        """Register a plugin"""
        self._plugins.append(plugin)
        logger.info(f"Registered plugin: {plugin.plugin_name}")

    def on_tool_registered(self, tool_def: 'ToolDefinition'):
        """Notify plugins of tool registration"""
        for plugin in self._plugins:
            try:
                plugin.on_tool_registered(tool_def)
            except Exception as e:
                logger.error(f"Plugin {plugin.plugin_name} error on_tool_registered: {e}")

    def on_tool_call_start(self, name: str, args: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Run pre-call hooks"""
        for plugin in self._plugins:
            try:
                intercept = plugin.on_tool_call_start(name, args)
                if intercept is not None:
                    return intercept
            except Exception as e:
                logger.error(f"Plugin {plugin.plugin_name} error on_tool_call_start: {e}")
        return None

    def on_tool_call_end(self, name: str, args: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
        """Run post-call hooks"""
        for plugin in self._plugins:
            try:
                result = plugin.on_tool_call_end(name, args, result)
            except Exception as e:
                logger.error(f"Plugin {plugin.plugin_name} error on_tool_call_end: {e}")
        return result

    def on_error(self, name: str, args: Dict[str, Any], error: Exception):
        """Notify plugins of errors"""
        for plugin in self._plugins:
            try:
                plugin.on_error(name, args, error)
            except Exception as e:
                logger.error(f"Plugin {plugin.plugin_name} error on_error: {e}")
```

**Example Plugins**:

1. **Analytics Plugin** (tracks usage):
```python
class AnalyticsPlugin(ToolPlugin):
    @property
    def plugin_name(self) -> str:
        return "analytics"

    def on_tool_call_end(self, name, args, result):
        # Track to analytics
        analytics.track_event('tool_call', {'tool': name, 'success': result.get('success')})
        return result
```

2. **Cache Plugin** (caches results):
```python
class CachePlugin(ToolPlugin):
    @property
    def plugin_name(self) -> str:
        return "cache"

    def on_tool_call_start(self, name, args):
        cache_key = self._get_cache_key(name, args)
        if cached := cache.get(cache_key):
            return cached
        return None

    def on_tool_call_end(self, name, args, result):
        cache_key = self._get_cache_key(name, args)
        cache.set(cache_key, result, ttl=300)
        return result
```

3. **Priority Plugin** (your use case):
```python
class PriorityPlugin(ToolPlugin):
    @property
    def plugin_name(self) -> str:
        return "priority"

    def __init__(self):
        self.priority_mgr = get_priority_manager()

    def on_tool_call_end(self, name, args, result):
        # Record usage
        self.priority_mgr.record_call(name, result.get('success'), ...)

        # Add priority metadata
        result['_priority_score'] = self.priority_mgr.calculate_priority(name)
        return result
```

**Integration**:
```python
# In pbixray_server_enhanced.py
from server.plugin_system import PluginManager
from plugins.analytics_plugin import AnalyticsPlugin
from plugins.priority_plugin import PriorityPlugin

plugin_manager = PluginManager()
plugin_manager.register_plugin(AnalyticsPlugin())
plugin_manager.register_plugin(PriorityPlugin())

# In call_tool()
intercept = plugin_manager.on_tool_call_start(name, arguments)
if intercept:
    return intercept

result = dispatcher.dispatch(name, arguments)
result = plugin_manager.on_tool_call_end(name, arguments, result)
```

---

### 4.4 Phase 4: Testing & Performance

#### Task 4.1: Comprehensive Testing

**Goal**: Achieve 60%+ test coverage

**Test Structure**:
```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_registry.py
â”‚   â”œâ”€â”€ test_dispatcher.py
â”‚   â”œâ”€â”€ test_middleware.py
â”‚   â”œâ”€â”€ test_priority_manager.py
â”‚   â”œâ”€â”€ test_plugin_system.py
â”‚   â””â”€â”€ handlers/
â”‚       â”œâ”€â”€ test_connection_handler.py
â”‚       â”œâ”€â”€ test_metadata_handler.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_full_workflow.py
â”‚   â”œâ”€â”€ test_measure_analysis.py
â”‚   â””â”€â”€ test_model_health_check.py
â”œâ”€â”€ performance/
â”‚   â”œâ”€â”€ test_benchmarks.py
â”‚   â””â”€â”€ test_load.py
â””â”€â”€ conftest.py
```

**Example Tests**:

1. **Unit Test: Priority Manager**:
```python
def test_priority_calculation():
    mgr = ToolPriorityManager()

    # Record usage
    mgr.record_call('list_tables', success=True, duration_ms=50, context={'type': 'metadata'})
    mgr.record_call('list_tables', success=True, duration_ms=45, context={'type': 'metadata'})

    # Calculate priority
    score = mgr.calculate_priority('list_tables', context={'type': 'metadata'})

    assert score > 0
    assert score < 100
```

2. **Integration Test: Full Workflow**:
```python
async def test_measure_analysis_workflow():
    # Connect
    result = await call_tool('detect_powerbi_desktop', {})
    assert result['success']

    result = await call_tool('connect_to_powerbi', {'model_index': 0})
    assert result['success']

    # Analyze measure
    result = await call_tool('get_measure_details', {
        'table': 'Sales',
        'measure': 'Total Revenue'
    })
    assert result['success']
    assert '_suggestions' in result  # Middleware active
```

3. **Performance Benchmark**:
```python
def test_list_tables_performance():
    iterations = 100
    start = time.time()

    for _ in range(iterations):
        call_tool('list_tables', {})

    duration = time.time() - start
    avg_ms = (duration / iterations) * 1000

    assert avg_ms < 50  # Should be under 50ms average
```

---

#### Task 4.2: Performance Optimizations

**Identified Hot Paths** (from analysis):
1. Tool dispatch and routing
2. JSON serialization
3. Token estimation
4. Cache lookups
5. Manager initialization

**Optimization Strategies**:

1. **Cache compiled patterns**:
```python
# In IntelligentToolRouter, compile regex once
class IntelligentToolRouter:
    def __init__(self):
        self.intent_patterns = self._build_intent_patterns()
        # Compile regex patterns once
        for pattern_group in self.intent_patterns:
            pattern_group['compiled_patterns'] = [
                re.compile(p) for p in pattern_group['patterns']
            ]
```

2. **Lazy JSON serialization**:
```python
# Only serialize when needed
class LazyJSONResponse:
    def __init__(self, data):
        self._data = data
        self._serialized = None

    def __str__(self):
        if self._serialized is None:
            self._serialized = json.dumps(self._data, indent=2)
        return self._serialized
```

3. **Async manager initialization**:
```python
# Initialize managers concurrently
async def initialize_managers():
    await asyncio.gather(
        init_query_executor(),
        init_bpa_analyzer(),
        init_dependency_analyzer()
    )
```

4. **Connection pooling**:
```python
# Reuse AMO connections
class ConnectionPool:
    def __init__(self, max_size=5):
        self._pool = []
        self._max_size = max_size

    def get_connection(self):
        if self._pool:
            return self._pool.pop()
        return create_new_connection()

    def return_connection(self, conn):
        if len(self._pool) < self._max_size:
            self._pool.append(conn)
```

**Expected Improvements**:
- Tool dispatch: 5-10ms â†’ 2-5ms
- List operations: 50ms â†’ 30ms
- Full analysis: 15s â†’ 10s

---

## 5. Performance Impact Analysis

### 5.1 Current Performance Baseline

| Operation | Current Latency | Target Latency | Improvement |
|-----------|----------------|----------------|-------------|
| detect_powerbi_desktop | 50-100ms | 30-60ms | 30% |
| list_tables | 50-80ms | 30-50ms | 35% |
| get_measure_details | 100-200ms | 70-140ms | 30% |
| run_dax (simple) | 150-300ms | 100-200ms | 30% |
| full_analysis | 15-45s | 10-30s | 30% |
| export_tmsl | 20-60s | 15-45s | 25% |

### 5.2 Performance Improvements by Phase

**Phase 1** (Intelligence Activation):
- **Latency Impact**: +5-10ms per request (middleware overhead)
- **Value**: High (better UX, suggestions, context)
- **Mitigation**: Cache middleware results, optimize pre/post processing

**Phase 2** (Tool Prioritization):
- **Latency Impact**: +2-5ms for priority calculation
- **Value**: High (better tool ordering)
- **Mitigation**: Cache priority scores, lazy calculation

**Phase 3** (Architecture Refactor):
- **Latency Impact**: -10-20ms (better caching, fewer locks)
- **Value**: High (maintainability)
- **Benefit**: Faster manager initialization, better concurrency

**Phase 4** (Performance Optimization):
- **Latency Impact**: -30-50ms (targeted optimizations)
- **Value**: High
- **Benefit**: Faster hot paths, async operations

**Net Impact**: **20-30% overall latency reduction** despite middleware overhead

---

## 6. Risk Assessment

### 6.1 Risk Matrix

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| Breaking changes during refactor | Medium | High | Feature flags, extensive testing, phased rollout |
| Performance regression | Low | Medium | Benchmarks, performance tests, monitoring |
| Plugin system misuse | Low | Medium | Documentation, examples, validation |
| Migration complexity | Medium | Medium | Clear docs, migration tools, support |
| Test coverage gaps | Medium | High | Code coverage tools, required tests in CI |

### 6.2 Mitigation Strategies

**For Breaking Changes**:
- Feature flags for all new features
- Backward compatibility layer for deprecated tools
- Clear migration guide
- Gradual deprecation (3-6 months)

**For Performance Regression**:
- Automated performance tests in CI
- Performance budgets (e.g., list_tables < 50ms)
- Profiling before/after changes
- Rollback plan if degradation detected

**For Plugin System**:
- Plugin validation and sandboxing
- Resource limits (CPU, memory, time)
- Clear plugin API documentation
- Example plugins as reference

---

## 7. Testing Strategy

### 7.1 Test Pyramid

```
              /\
             /  \    E2E Tests (10%)
            /----\   - Full workflows
           /      \  - User scenarios
          /--------\
         /          \ Integration Tests (30%)
        /            \ - Handler integration
       /--------------\ - Middleware flow
      /                \ - Plugin system
     /------------------\
    /                    \ Unit Tests (60%)
   /                      \ - Individual functions
  /------------------------\ - Managers, utils
 /                          \ - Priority calculation
/____________________________\
```

### 7.2 Coverage Goals

- **Unit Tests**: 60% minimum, 80% target
- **Integration Tests**: All critical workflows
- **Performance Tests**: Baseline + regression
- **E2E Tests**: Top 10 user scenarios

### 7.3 CI/CD Pipeline

```yaml
# .github/workflows/test.yml
name: Test Suite

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install -r requirements-dev.txt

      - name: Run unit tests
        run: pytest tests/unit --cov=. --cov-report=xml

      - name: Run integration tests
        run: pytest tests/integration

      - name: Run performance tests
        run: pytest tests/performance --benchmark-only

      - name: Coverage check
        run: |
          coverage report --fail-under=60

      - name: Upload coverage
        uses: codecov/codecov-action@v2
```

---

## 8. Migration & Rollout

### 8.1 Phased Rollout Strategy

**Week 1-2: Phase 1 (Intelligence)**
- âœ… Deploy with feature flag disabled
- âœ… Enable for internal testing
- âœ… Collect feedback
- âœ… Enable for 10% of requests
- âœ… Enable for 50% of requests
- âœ… Enable for 100% of requests

**Week 3-4: Phase 2 (Prioritization)**
- âœ… Deploy priority system
- âœ… Monitor tool ordering changes
- âœ… Validate user satisfaction
- âœ… Add deprecation warnings for legacy tools

**Week 5-7: Phase 3 (Architecture)**
- âœ… Deploy service container
- âœ… Migrate handlers gradually
- âœ… Monitor performance metrics
- âœ… Complete migration

**Week 8-10: Phase 4 (Performance)**
- âœ… Deploy optimizations
- âœ… Validate performance improvements
- âœ… Achieve test coverage goals

### 8.2 Rollback Plan

Each phase has independent rollback via feature flags:

```json
{
  "features": {
    "enable_intelligent_middleware": true,    // Phase 1
    "enable_tool_prioritization": true,       // Phase 2
    "enable_service_container": false,        // Phase 3 (not yet)
    "enable_plugin_system": false             // Phase 3
  }
}
```

If issues detected:
1. Disable feature flag
2. Monitor for stability
3. Investigate root cause
4. Fix and re-enable

---

## 9. Success Metrics

### 9.1 Performance Metrics

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| Avg response time | 150ms | 100ms | Logging, monitoring |
| P95 response time | 500ms | 350ms | Logging, monitoring |
| Full analysis time | 30s | 20s | Benchmarks |
| Cache hit rate | 60% | 80% | Cache metrics |

### 9.2 Quality Metrics

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| Test coverage | ~1% | 60% | Coverage tools |
| Bug rate | Baseline | -50% | Issue tracker |
| Tool consolidation | 60 tools | 45 tools | Registry count |
| Deprecated tools | 0 | 15 | Deprecation tracking |

### 9.3 User Experience Metrics

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| Suggestions shown | 0% | 80% | Analytics |
| Suggested tools used | 0% | 30% | Analytics |
| Context tracking active | 0% | 90% | Analytics |
| User satisfaction | N/A | Survey | User feedback |

### 9.4 Developer Experience Metrics

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| New tool creation time | 2-3 days | 4-6 hours | Time tracking |
| Build time | N/A | <5 min | CI metrics |
| Test run time | <1 min | <2 min | CI metrics |

---

## Appendix A: File Change Summary

### Files to Create

```
server/
â”œâ”€â”€ tool_priority.py (new)
â”œâ”€â”€ plugin_system.py (new)

core/infrastructure/
â”œâ”€â”€ service_container.py (new)

plugins/ (new directory)
â”œâ”€â”€ __init__.py
â”œâ”€â”€ analytics_plugin.py
â”œâ”€â”€ priority_plugin.py
â””â”€â”€ cache_plugin.py

tests/ (expanded)
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_priority_manager.py (new)
â”‚   â”œâ”€â”€ test_plugin_system.py (new)
â”‚   â”œâ”€â”€ test_service_container.py (new)
â”‚   â””â”€â”€ handlers/ (new, 26 files)
â”œâ”€â”€ integration/ (new)
â”‚   â”œâ”€â”€ test_full_workflow.py
â”‚   â”œâ”€â”€ test_measure_analysis.py
â”‚   â””â”€â”€ test_model_health_check.py
â””â”€â”€ performance/ (new)
    â”œâ”€â”€ test_benchmarks.py
    â””â”€â”€ test_load.py

docs/
â”œâ”€â”€ TOOL_MIGRATION.md (new)
â”œâ”€â”€ PLUGIN_GUIDE.md (new)
â””â”€â”€ ARCHITECTURE.md (new)
```

### Files to Modify

```
src/
â”œâ”€â”€ pbixray_server_enhanced.py (middleware integration)

server/
â”œâ”€â”€ registry.py (priority sorting, deprecation)
â”œâ”€â”€ dispatch.py (plugin hooks)
â”œâ”€â”€ intelligent_middleware.py (activation)

core/infrastructure/
â”œâ”€â”€ connection_state.py (gradual reduction)

core/config/
â”œâ”€â”€ default_config.json (new feature flags)
```

### Files to Deprecate (Eventually Remove)

```
server/handlers/
â”œâ”€â”€ (individual operation handlers - migrate to consolidated)
```

---

## Appendix B: Configuration Reference

### New Configuration Options

```json
{
  "features": {
    "enable_intelligent_middleware": true,
    "enable_intelligent_routing": true,
    "enable_tool_prioritization": true,
    "enable_context_tracking": true,
    "enable_suggestion_engine": true,
    "enable_plugin_system": true,
    "enable_service_container": false
  },

  "priority": {
    "usage_weight": 0.25,
    "context_weight": 0.25,
    "recency_weight": 0.10,
    "success_weight": 0.10,
    "base_weight": 0.30
  },

  "plugins": {
    "enabled": ["analytics", "priority", "cache"],
    "disabled": []
  },

  "performance": {
    "enable_response_streaming": false,
    "enable_connection_pooling": true,
    "pool_size": 5,
    "async_manager_init": true
  }
}
```

---

## Appendix C: Plugin Development Guide

### Creating a Custom Plugin

```python
# plugins/my_custom_plugin.py
from server.plugin_system import ToolPlugin
from typing import Dict, Any, Optional

class MyCustomPlugin(ToolPlugin):
    @property
    def plugin_name(self) -> str:
        return "my_custom_plugin"

    def on_tool_call_start(self, name: str, args: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Called before tool execution
        Return dict to intercept and short-circuit execution
        """
        # Example: Block certain tools
        if name == 'dangerous_tool' and not self.is_authorized(args):
            return {
                'success': False,
                'error': 'Not authorized',
                'error_type': 'authorization_error'
            }

        # Return None to proceed normally
        return None

    def on_tool_call_end(self, name: str, args: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Called after tool execution
        Modify result before returning to user
        """
        # Example: Add custom metadata
        result['_custom_metadata'] = {
            'processed_by': self.plugin_name,
            'timestamp': time.time()
        }
        return result

    def on_error(self, name: str, args: Dict[str, Any], error: Exception):
        """Called when tool execution fails"""
        # Example: Log errors to custom system
        logger.error(f"Tool {name} failed: {error}")

# Register the plugin
from server.plugin_system import get_plugin_manager
get_plugin_manager().register_plugin(MyCustomPlugin())
```

---

## Appendix D: Priority Calculation Examples

### Example 1: Fresh Server (No Usage History)

Tool: `list_tables`
- Base score (sort_order=10): 30.0 (top priority)
- Usage score: 0.0 (no history)
- Context score: 5.0 (neutral)
- Recency score: 0.0 (never used)
- Success score: 5.0 (neutral)
- **Total: 40.0**

Tool: `export_tmsl`
- Base score (sort_order=700): 9.0 (lower priority)
- Usage score: 0.0
- Context score: 5.0
- Recency score: 0.0
- Success score: 5.0
- **Total: 19.0**

**Result**: `list_tables` ranked higher (expected behavior)

---

### Example 2: After Heavy Usage

User frequently exports TMSL, rarely lists tables.

Tool: `list_tables`
- Base score: 30.0
- Usage score: 2.0 (low usage)
- Context score: 5.0
- Recency score: 0.0 (not recent)
- Success score: 5.0
- **Total: 42.0**

Tool: `export_tmsl`
- Base score: 9.0
- Usage score: 25.0 (heavy usage)
- Context score: 20.0 (used in export context)
- Recency score: 10.0 (just used)
- Success score: 10.0 (always succeeds)
- **Total: 74.0**

**Result**: `export_tmsl` now ranked HIGHER than `list_tables` (learned from usage)

---

### Example 3: Context-Aware Prioritization

User working on measures (context: `{type: 'measure'}`).

Tool: `list_tables`
- Base score: 30.0
- Usage score: 10.0
- Context score: 3.0 (not relevant to measures)
- Recency score: 0.0
- Success score: 8.0
- **Total: 51.0**

Tool: `get_measure_details`
- Base score: 25.0
- Usage score: 15.0
- Context score: 25.0 (highly relevant to measure context)
- Recency score: 10.0
- Success score: 9.0
- **Total: 84.0**

**Result**: Measure tools prioritized when working on measures

---

## Summary

This refactoring plan provides a comprehensive roadmap for improving the MCP-PowerBI-Finvision server across four key dimensions:

1. **Structure**: Cleaner architecture with dependency injection, plugin system, and reduced global state
2. **Performance**: 20-30% faster through optimizations and better caching
3. **Extensibility**: Easy to add features like tool prioritization through plugins
4. **Quality**: 60%+ test coverage and robust CI/CD pipeline

The plan is designed to be:
- âœ… **Phased**: Each phase delivers value independently
- âœ… **Low-risk**: Feature flags, backward compatibility, gradual rollout
- âœ… **Measurable**: Clear success metrics and benchmarks
- âœ… **Practical**: Based on existing codebase analysis, not theoretical

**Your specific use case** (tool prioritization based on request context) becomes **trivial to implement** after Phase 2, requiring only:
1. Create priority plugin (30 minutes)
2. Register plugin (5 minutes)
3. Enable feature flag (1 minute)

**Total implementation time for your feature: ~1 hour** (vs. several days with current architecture)

---

**Next Steps**:
1. Review this plan and provide feedback
2. Prioritize phases based on your needs
3. Begin Phase 1 implementation (2 weeks)
4. Iterate and adjust based on results
