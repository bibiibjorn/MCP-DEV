"""
Analysis Handler
Handles model analysis tools including simple analysis, full analysis, BPA, performance, and validation
"""
from typing import Dict, Any
import logging
from server.registry import ToolDefinition
from core.infrastructure.connection_state import connection_state
from core.validation.error_handler import ErrorHandler
from core.utilities.business_impact import enrich_issue_with_impact, add_impact_summary

logger = logging.getLogger(__name__)

def _extract_operation_summary(op_id: str, op_result: Dict[str, Any]) -> str:
    """
    Extract a concise success message from operation result.
    Optimized helper to avoid repetitive if-elif chains.
    """
    if op_id == '01_database':
        db_data = op_result.get('data', [{}])[0]
        return f"[OK] SUCCESS: {db_data.get('name', 'N/A')} (Compatibility: {db_data.get('compatibilityLevel', 0)})"
    elif op_id == '02_stats':
        counts = op_result.get('counts', {})
        return f"[OK] SUCCESS: {counts.get('tables', 0)} tables, {counts.get('measures', 0)} measures, {counts.get('columns', 0)} columns"
    elif op_id == '03_tables':
        return f"[OK] SUCCESS: Found {op_result.get('table_count', 0)} tables"
    elif op_id == '04_measures':
        return f"[OK] SUCCESS: Retrieved {len(op_result.get('data', []))} measures"
    elif op_id == '05_columns':
        data = op_result.get('data', [])
        col_count = sum(len(t.get('columns', [])) for t in data)
        return f"[OK] SUCCESS: Retrieved {col_count} columns across {len(data)} tables"
    elif op_id == '06_relationships':
        return f"[OK] SUCCESS: Found {len(op_result.get('data', []))} relationships"
    elif op_id == '07_calculation_groups':
        data = op_result.get('data', [])
        total_items = sum(len(cg.get('calculationItems', [])) for cg in data)
        return f"[OK] SUCCESS: Found {len(data)} calculation groups with {total_items} items"
    elif op_id == '08_roles':
        return f"[OK] SUCCESS: Found {len(op_result.get('data', []))} security roles"
    else:
        return "[OK] SUCCESS: Operation completed"

def _format_analysis_to_log(op_analysis: Dict[str, Any], op_name: str, op_time: float) -> list:
    """
    Format operation analysis into execution log lines.
    Optimized to consolidate formatting logic.
    """
    log_lines = ["", f"   === ANALYSIS SUMMARY FOR {op_name.upper()} ==="]

    if op_analysis.get('key_findings'):
        log_lines.append("   Key Findings:")
        log_lines.extend([f"     * {finding}" for finding in op_analysis['key_findings']])

    if op_analysis.get('insights'):
        log_lines.append("   Insights:")
        log_lines.extend([f"     - {insight}" for insight in op_analysis['insights']])

    if op_analysis.get('recommendations'):
        log_lines.append("   Recommendations:")
        log_lines.extend([f"     ! {rec}" for rec in op_analysis['recommendations']])

    log_lines.append(f"   [TIME] Execution time: {op_time}s")
    log_lines.append("   " + "=" * 70)

    return log_lines

def _generate_operation_analysis(op_id: str, result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate AI-optimized analysis summary for each operation.
    Provides insights, patterns, and recommendations specific to each operation.
    """
    if not result.get('success'):
        return {
            'status': 'failed',
            'insights': [],
            'recommendations': []
        }

    analysis = {
        'status': 'success',
        'insights': [],
        'recommendations': [],
        'key_findings': []
    }

    try:
        # Database operation analysis
        if op_id == '01_database':
            data = result.get('data', [{}])[0]
            compat = data.get('compatibilityLevel', 0)
            size_mb = round(data.get('estimatedSize', 0) / (1024 * 1024), 2) if data.get('estimatedSize') else 0
            db_name = data.get('name', 'Unknown')
            db_id = data.get('id', 'Unknown')

            analysis['key_findings'].append(f"Database: {db_name}")
            analysis['key_findings'].append(f"Compatibility Level: {compat}")

            # Detailed compatibility level interpretation
            if compat >= 1604:
                analysis['insights'].append(f"âœ… Latest Power BI compatibility level ({compat})")
                analysis['insights'].append("   Supports: Calculation groups, enhanced refresh, DirectQuery improvements")
            elif compat >= 1600:
                analysis['insights'].append(f"âœ… Power BI Desktop format ({compat})")
                analysis['insights'].append("   Supports: Calculation groups, object-level security")
            elif compat >= 1500:
                analysis['insights'].append(f"â„¹ï¸ Azure Analysis Services format ({compat})")
                analysis['insights'].append("   Consider upgrading to 1600+ for Power BI features")
            elif compat >= 1400:
                analysis['insights'].append(f"âš ï¸ SQL Server 2017 format ({compat})")
                analysis['recommendations'].append("Upgrade compatibility level to access modern Power BI features")
            else:
                analysis['insights'].append(f"âš ï¸ Legacy format ({compat})")
                analysis['recommendations'].append("IMPORTANT: Upgrade compatibility level for better performance and features")

            # Size analysis with detailed recommendations
            if size_mb > 0:
                analysis['key_findings'].append(f"Model size: {size_mb} MB")

                if size_mb > 2000:
                    size_category = "Very Large (>2GB)"
                    analysis['insights'].append(f"ðŸ”´ {size_category} - Performance critical")
                    analysis['recommendations'].append("CRITICAL: Implement incremental refresh, aggregations, and column optimization")
                    analysis['recommendations'].append("Consider splitting into multiple models or using composite models")
                elif size_mb > 1000:
                    size_category = "Large (1-2GB)"
                    analysis['insights'].append(f"ðŸŸ¡ {size_category} - Monitor performance")
                    analysis['recommendations'].append("Implement incremental refresh for fact tables")
                    analysis['recommendations'].append("Review and remove unused columns and high-cardinality text fields")
                elif size_mb > 500:
                    size_category = "Medium (500MB-1GB)"
                    analysis['insights'].append(f"ðŸŸ¢ {size_category} - Good range")
                    analysis['recommendations'].append("Monitor refresh times and consider optimization strategies")
                elif size_mb > 100:
                    size_category = "Small-Medium (100-500MB)"
                    analysis['insights'].append(f"âœ… {size_category} - Optimal size")
                else:
                    size_category = "Small (<100MB)"
                    analysis['insights'].append(f"âœ… {size_category} - Excellent performance expected")

        # Stats operation analysis
        elif op_id == '02_stats':
            counts = result.get('counts', {})
            tables = counts.get('tables', 0)
            measures = counts.get('measures', 0)
            columns = counts.get('columns', 0)
            rels = counts.get('relationships', 0)
            calc_groups = counts.get('calculation_groups', 0)
            roles = counts.get('roles', 0)
            partitions = counts.get('partitions', 0)

            analysis['key_findings'] = [
                f"ðŸ“‹ {tables} tables",
                f"ðŸ“ {columns} columns",
                f"ðŸ“ {measures} measures",
                f"ðŸ”— {rels} relationships"
            ]

            if calc_groups > 0:
                analysis['key_findings'].append(f"âš¡ {calc_groups} calculation groups")
            if roles > 0:
                analysis['key_findings'].append(f"ðŸ”’ {roles} security roles")

            # Complexity assessment
            complexity_score = 0
            if tables > 100:
                complexity_score += 2
                analysis['insights'].append("âš ï¸ Large model with many tables - ensure proper naming conventions")
            elif tables > 50:
                complexity_score += 1
                analysis['insights'].append("â„¹ï¸ Moderate table count - good organization is important")
            else:
                analysis['insights'].append("âœ… Manageable table count")

            if measures > 500:
                complexity_score += 3
                analysis['insights'].append("âš ï¸ Very high measure count - display folders are critical")
                analysis['recommendations'].append("IMPORTANT: Organize measures with hierarchical display folders")
            elif measures > 200:
                complexity_score += 2
                analysis['insights'].append("âš ï¸ High measure count - use display folders for organization")
                analysis['recommendations'].append("Ensure measures are organized in logical display folder hierarchies")
            elif measures > 100:
                complexity_score += 1
                analysis['insights'].append("â„¹ï¸ Moderate measure count - maintain good organization")

            if calc_groups > 0:
                analysis['insights'].append(f"âœ… Using {calc_groups} calculation groups - advanced DAX pattern")
                analysis['insights'].append("   This indicates sophisticated time intelligence or scenario analysis")

            # Ratio analysis with interpretation
            if tables > 0:
                cols_per_table = round(columns / tables, 1)
                measures_per_table = round(measures / tables, 1)
                rels_per_table = round(rels / tables, 1)

                analysis['insights'].append(f"Model ratios:")
                analysis['insights'].append(f"   â€¢ Avg {cols_per_table} columns per table")
                analysis['insights'].append(f"   â€¢ Avg {measures_per_table} measures per table")
                analysis['insights'].append(f"   â€¢ Avg {rels_per_table} relationships per table")

                # Interpret ratios
                if cols_per_table > 50:
                    analysis['recommendations'].append("High column-to-table ratio - review for denormalization or unused columns")
                elif cols_per_table < 5:
                    analysis['insights'].append("   â„¹ï¸ Low column count per table - may indicate highly normalized structure")

                if measures_per_table > 30:
                    analysis['insights'].append("   â„¹ï¸ Many measures per table - likely using dedicated measure tables (good!)")

            # Overall complexity assessment
            if complexity_score >= 5:
                analysis['insights'].append("ðŸ”´ Model Complexity: HIGH - requires strong governance and organization")
            elif complexity_score >= 3:
                analysis['insights'].append("ðŸŸ¡ Model Complexity: MEDIUM - good practices important")
            else:
                analysis['insights'].append("ðŸŸ¢ Model Complexity: LOW - easy to maintain")

            # Security and partitioning insights
            if roles > 0:
                analysis['insights'].append(f"âœ… Security: {roles} RLS roles implemented")
            else:
                analysis['insights'].append("â„¹ï¸ No RLS roles - data is accessible to all users")

            if partitions > tables:
                analysis['insights'].append(f"â„¹ï¸ {partitions} partitions across {tables} tables - likely using incremental refresh")

        # Tables operation analysis
        elif op_id == '03_tables':
            data = result.get('data', [])
            table_count = len(data)

            # Identify measure tables
            measure_tables = [t for t in data if t.get('measureCount', 0) > 10 and t.get('columnCount', 0) < 5]
            large_tables = [t for t in data if t.get('columnCount', 0) > 50]
            hidden_tables = [t for t in data if t.get('isHidden', False)]

            # Classify tables by type (fact, dimension, etc.)
            fact_tables = [t for t in data if 'fact' in t.get('name', '').lower()]
            dim_tables = [t for t in data if any(prefix in t.get('name', '').lower() for prefix in ['dim', 'dimension'])]

            # Calculate statistics
            total_columns = sum(t.get('columnCount', 0) for t in data)
            total_measures = sum(t.get('measureCount', 0) for t in data)
            avg_cols_per_table = round(total_columns / table_count, 1) if table_count > 0 else 0

            analysis['key_findings'].append(f"{table_count} tables in model")
            analysis['key_findings'].append(f"{total_columns} total columns (avg {avg_cols_per_table} per table)")
            analysis['key_findings'].append(f"{total_measures} total measures distributed across tables")

            if measure_tables:
                measure_table_names = [t.get('name', '') for t in measure_tables[:3]]
                analysis['insights'].append(f"âœ… {len(measure_tables)} dedicated measure tables (best practice)")
                analysis['insights'].append(f"   Examples: {', '.join(measure_table_names)}")

            if fact_tables:
                analysis['insights'].append(f"Found {len(fact_tables)} fact tables (likely transactional data)")
            if dim_tables:
                analysis['insights'].append(f"Found {len(dim_tables)} dimension tables (lookup/reference data)")

            if large_tables:
                large_table_details = [f"{t.get('name', '')} ({t.get('columnCount', 0)} cols)" for t in large_tables[:3]]
                analysis['insights'].append(f"âš ï¸ {len(large_tables)} tables with >50 columns")
                analysis['insights'].append(f"   Examples: {', '.join(large_table_details)}")
                analysis['recommendations'].append("Review large tables for potential normalization or column hiding")

            if hidden_tables:
                analysis['insights'].append(f"â„¹ï¸ {len(hidden_tables)} hidden tables (good for backend calculations)")

        # Measures operation analysis
        elif op_id == '04_measures':
            data = result.get('data', [])
            measure_count = len(data)

            # Analyze display folders
            with_folders = [m for m in data if m.get('displayFolder')]
            folder_usage = round(len(with_folders) / measure_count * 100, 1) if measure_count > 0 else 0

            # Analyze folder structure
            folders = set()
            top_level_folders = set()
            for m in data:
                folder = m.get('displayFolder', '')
                if folder:
                    folders.add(folder)
                    top_level_folders.add(folder.split('\\')[0])

            # Detect naming patterns and measure types
            time_intel_measures = [m for m in data if any(x in m.get('name', '').lower() for x in ['ytd', 'mtd', 'qtd', 'py', 'ly', 'yoy', 'mom'])]
            calc_measures = [m for m in data if any(x in m.get('name', '').lower() for x in ['calc', 'calculated'])]
            base_measures = [m for m in data if 'base' in m.get('name', '').lower()]
            kpi_measures = [m for m in data if 'kpi' in m.get('name', '').lower()]

            # Group measures by table
            measures_by_table = {}
            for m in data:
                table = m.get('table', 'Unknown')
                measures_by_table[table] = measures_by_table.get(table, 0) + 1

            top_tables = sorted(measures_by_table.items(), key=lambda x: x[1], reverse=True)[:5]

            analysis['key_findings'].append(f"{measure_count} measures analyzed")
            analysis['key_findings'].append(f"{len(folders)} unique display folders used")
            analysis['key_findings'].append(f"{len(top_level_folders)} top-level folder categories")

            # Organization assessment
            if folder_usage > 80:
                analysis['insights'].append(f"âœ… Excellent organization: {folder_usage}% measures use display folders")
            elif folder_usage > 50:
                analysis['insights'].append(f"âœ”ï¸ Good organization: {folder_usage}% measures use display folders")
                analysis['recommendations'].append(f"Improve organization: {100-folder_usage}% of measures lack display folders")
            else:
                analysis['insights'].append(f"âš ï¸ Poor organization: Only {folder_usage}% measures use display folders")
                analysis['recommendations'].append("IMPORTANT: Organize measures with display folders for better user experience")

            # Pattern analysis
            if time_intel_measures:
                examples = [m.get('name', '') for m in time_intel_measures[:3]]
                analysis['insights'].append(f"Found {len(time_intel_measures)} time intelligence measures")
                analysis['insights'].append(f"   Examples: {', '.join(examples)}")

            if calc_measures or base_measures:
                total_calc = len(calc_measures) + len(base_measures)
                analysis['insights'].append(f"Found {total_calc} calculated/base measures (good DAX layering pattern)")

            if kpi_measures:
                analysis['insights'].append(f"Found {len(kpi_measures)} KPI measures")

            # Table distribution
            analysis['insights'].append(f"Measures distributed across {len(measures_by_table)} tables")
            if top_tables:
                top_table_info = [f"{table} ({count})" for table, count in top_tables[:3]]
                analysis['insights'].append(f"   Top tables: {', '.join(top_table_info)}")

            # Recommendations
            if len(measures_by_table) > 10 and not any('measure' in table.lower() for table in measures_by_table.keys()):
                analysis['recommendations'].append("Consider consolidating measures into dedicated measure tables")

        # Columns operation analysis
        elif op_id == '05_columns':
            data = result.get('data', [])
            total_columns = sum(len(t.get('columns', [])) for t in data)
            table_count = len(data)

            # Extract all columns
            all_columns = [col for table in data for col in table.get('columns', [])]

            # Data type analysis
            data_types = {}
            for col in all_columns:
                dt = col.get('dataType', 'Unknown')
                data_types[dt] = data_types.get(dt, 0) + 1

            # Identify calculated columns vs regular columns
            calculated_cols = [col for col in all_columns if col.get('type') == 'calculated']
            regular_cols = [col for col in all_columns if col.get('type') != 'calculated']

            # Identify hidden columns
            hidden_cols = [col for col in all_columns if col.get('isHidden', False)]

            # Find key columns (common patterns)
            key_cols = [col for col in all_columns if any(x in col.get('name', '').lower() for x in ['key', 'id', 'sk', 'pk'])]
            date_cols = [col for col in all_columns if col.get('dataType') == 'DateTime']

            analysis['key_findings'].append(f"{total_columns} columns across {table_count} tables")
            if calculated_cols:
                calc_pct = round(len(calculated_cols) / total_columns * 100, 1)
                analysis['key_findings'].append(f"{len(calculated_cols)} calculated columns ({calc_pct}%)")

            # Data type breakdown
            if data_types:
                data_type_summary = []
                sorted_types = sorted(data_types.items(), key=lambda x: x[1], reverse=True)
                for dt, count in sorted_types[:5]:
                    pct = round(count / total_columns * 100, 1)
                    data_type_summary.append(f"{dt}: {count} ({pct}%)")

                analysis['insights'].append(f"Data type distribution:")
                for dt_info in data_type_summary:
                    analysis['insights'].append(f"   â€¢ {dt_info}")

            # Column patterns
            if hidden_cols:
                hidden_pct = round(len(hidden_cols) / total_columns * 100, 1)
                analysis['insights'].append(f"âœ… {len(hidden_cols)} hidden columns ({hidden_pct}%) - good for data cleanliness")

            if key_cols:
                analysis['insights'].append(f"Found {len(key_cols)} key/ID columns for relationships")

            if date_cols:
                date_table_names = set([col.get('table', '') for col in date_cols])
                analysis['insights'].append(f"Found {len(date_cols)} date/time columns across {len(date_table_names)} tables")

            # Calculated column warnings
            if calculated_cols:
                if len(calculated_cols) > 50:
                    analysis['recommendations'].append("âš ï¸ HIGH: Many calculated columns detected - consider moving logic to source or using measures")
                elif len(calculated_cols) > 20:
                    analysis['recommendations'].append("Review calculated columns - some may be better as measures for performance")
                else:
                    analysis['insights'].append(f"â„¹ï¸ {len(calculated_cols)} calculated columns (reasonable amount)")

        # Relationships operation analysis - ENHANCED with detailed cardinality patterns
        elif op_id == '06_relationships':
            data = result.get('data', [])
            total_rels = len(data)

            # Analyze ALL cardinality patterns
            one_to_one = [r for r in data if r.get('fromCardinality') == 'One' and r.get('toCardinality') == 'One']
            one_to_many = [r for r in data if r.get('fromCardinality') == 'One' and r.get('toCardinality') == 'Many']
            many_to_one = [r for r in data if r.get('fromCardinality') == 'Many' and r.get('toCardinality') == 'One']
            many_to_many = [r for r in data if r.get('fromCardinality') == 'Many' and r.get('toCardinality') == 'Many']

            bidirectional = [r for r in data if r.get('crossFilteringBehavior') == 'BothDirections']
            inactive = [r for r in data if not r.get('isActive')]

            # Key findings with cardinality breakdown
            analysis['key_findings'].append(f"{total_rels} relationships total")

            cardinality_breakdown = []
            if many_to_one:
                cardinality_breakdown.append(f"{len(many_to_one)} Many:One (standard)")
            if one_to_many:
                cardinality_breakdown.append(f"{len(one_to_many)} One:Many")
            if many_to_many:
                cardinality_breakdown.append(f"{len(many_to_many)} Many:Many")
            if one_to_one:
                cardinality_breakdown.append(f"{len(one_to_one)} One:One")

            if cardinality_breakdown:
                analysis['key_findings'].append("Cardinality: " + ", ".join(cardinality_breakdown))

            # PROMINENT warnings for problematic patterns
            if many_to_many:
                analysis['insights'].append(f"âš ï¸ {len(many_to_many)} MANY-TO-MANY relationships detected")

                # Show examples of M:M relationships
                m2m_examples = [f"{r.get('fromTable')} â†” {r.get('toTable')}" for r in many_to_many[:3]]
                if m2m_examples:
                    analysis['insights'].append(f"   Examples: {', '.join(m2m_examples)}")

                if len(many_to_many) > 10:
                    analysis['recommendations'].append("âš ï¸ HIGH: Consider bridge tables for M:M relationships to improve performance")
                elif len(many_to_many) > 5:
                    analysis['recommendations'].append("MEDIUM: Review M:M relationships for potential optimization")
                else:
                    analysis['recommendations'].append("LOW: Monitor M:M relationship performance")

            if bidirectional:
                analysis['insights'].append(f"âš ï¸ {len(bidirectional)} BI-DIRECTIONAL relationships found")

                # Show examples of bidirectional relationships
                bidir_examples = [f"{r.get('fromTable')} â†” {r.get('toTable')}" for r in bidirectional[:3]]
                if bidir_examples:
                    analysis['insights'].append(f"   Examples: {', '.join(bidir_examples)}")

                analysis['recommendations'].append("âš ï¸ Review bidirectional filters - can cause ambiguity and performance issues")

            if inactive:
                analysis['insights'].append(f"â„¹ï¸ {len(inactive)} INACTIVE relationships (likely used with USERELATIONSHIP)")

                # Show examples of inactive relationships
                inactive_examples = [f"{r.get('fromTable')} â†’ {r.get('toTable')}" for r in inactive[:3]]
                if inactive_examples:
                    analysis['insights'].append(f"   Examples: {', '.join(inactive_examples)}")

            # Positive insights
            if not many_to_many and not bidirectional:
                analysis['insights'].append("âœ… Clean relationship model - no M:M or bidirectional relationships")

            if many_to_one and len(many_to_one) == total_rels - len(inactive):
                analysis['insights'].append("âœ… Standard star schema pattern - all active relationships are Many:One")

        # Calculation groups analysis
        elif op_id == '07_calculation_groups':
            data = result.get('data', [])
            group_count = len(data)
            total_items = sum(len(cg.get('calculationItems', [])) for cg in data)

            if group_count > 0:
                analysis['key_findings'].append(f"{group_count} calculation groups with {total_items} items total")

                # Detailed group analysis
                group_details = []
                time_groups = []
                currency_groups = []
                scenario_groups = []

                for cg in data:
                    name = cg.get('name', '')
                    items = cg.get('calculationItems', [])
                    item_count = len(items)
                    item_names = [item.get('name', '') for item in items[:5]]

                    group_details.append(f"{name} ({item_count} items)")

                    # Categorize by pattern
                    name_lower = name.lower()
                    if any(x in name_lower for x in ['time', 'date', 'period', 'ytd', 'mtd']):
                        time_groups.append(name)
                    if any(x in name_lower for x in ['currency', 'fx', 'exchange']):
                        currency_groups.append(name)
                    if any(x in name_lower for x in ['scenario', 'variance', 'comparison']):
                        scenario_groups.append(name)

                analysis['insights'].append("âœ… Advanced DAX implementation with calculation groups")

                # Show group details
                if len(group_details) <= 5:
                    for detail in group_details:
                        analysis['insights'].append(f"   â€¢ {detail}")
                else:
                    for detail in group_details[:3]:
                        analysis['insights'].append(f"   â€¢ {detail}")
                    analysis['insights'].append(f"   â€¢ ... and {len(group_details) - 3} more")

                # Pattern analysis
                patterns_found = []
                if time_groups:
                    patterns_found.append(f"Time intelligence ({len(time_groups)})")
                if currency_groups:
                    patterns_found.append(f"Currency conversion ({len(currency_groups)})")
                if scenario_groups:
                    patterns_found.append(f"Scenario analysis ({len(scenario_groups)})")

                if patterns_found:
                    analysis['insights'].append(f"Patterns detected: {', '.join(patterns_found)}")

                # Recommendations
                if total_items < 10:
                    analysis['insights'].append("â„¹ï¸ Simple calculation group setup - good for focused scenarios")
                elif total_items > 30:
                    analysis['insights'].append("âœ… Comprehensive calculation group implementation")
                    analysis['recommendations'].append("Ensure calculation items are well-documented for team understanding")

            else:
                analysis['insights'].append("â„¹ï¸ No calculation groups found")
                analysis['recommendations'].append("Consider calculation groups for time intelligence (YTD, MTD, PY) to reduce measure count")

        # Roles analysis
        elif op_id == '08_roles':
            data = result.get('data', [])
            role_count = len(data)

            if role_count > 0:
                analysis['key_findings'].append(f"{role_count} security roles configured")

                # Analyze role details
                with_permissions = [r for r in data if r.get('tablePermissionCount', 0) > 0]
                read_only = [r for r in data if r.get('modelPermission', '') == 'Read']
                admin_roles = [r for r in data if r.get('modelPermission', '') == 'Administrator']

                total_permissions = sum(r.get('tablePermissionCount', 0) for r in data)

                analysis['insights'].append("âœ… Row-level security (RLS) is implemented")

                # Role details
                role_names = [r.get('name', '') for r in data[:5]]
                if len(role_names) <= 5:
                    analysis['insights'].append(f"Roles: {', '.join(role_names)}")
                else:
                    analysis['insights'].append(f"Roles: {', '.join(role_names[:3])}, ... and {role_count - 3} more")

                # Permission analysis
                if with_permissions:
                    avg_perms = round(total_permissions / len(with_permissions), 1)
                    analysis['insights'].append(f"{len(with_permissions)} roles with table-level filters")
                    analysis['insights'].append(f"   â€¢ Average {avg_perms} table permissions per role")
                    analysis['insights'].append(f"   â€¢ {total_permissions} total table permissions across all roles")

                    # Security complexity assessment
                    if total_permissions > 30:
                        analysis['insights'].append("ðŸŸ¡ Complex RLS implementation - ensure thorough testing")
                        analysis['recommendations'].append("Document RLS logic and test with different user contexts")
                    elif total_permissions > 10:
                        analysis['insights'].append("â„¹ï¸ Moderate RLS complexity")
                        analysis['recommendations'].append("Test RLS with representative user accounts")
                    else:
                        analysis['insights'].append("âœ… Simple RLS implementation - easy to maintain")
                else:
                    analysis['insights'].append("âš ï¸ Roles exist but no table permissions defined")
                    analysis['recommendations'].append("Define table permissions for roles to enforce data security")

                # Permission types
                if read_only:
                    analysis['insights'].append(f"â„¹ï¸ {len(read_only)} read-only roles (standard pattern)")
                if admin_roles:
                    analysis['insights'].append(f"â„¹ï¸ {len(admin_roles)} administrator roles")

                # Best practices
                if role_count > 20:
                    analysis['recommendations'].append("Many roles defined - consider role consolidation or dynamic RLS with user tables")
                elif role_count > 10:
                    analysis['insights'].append("â„¹ï¸ Multiple roles - good for different user groups")

            else:
                analysis['insights'].append("â„¹ï¸ No security roles - model is open to all users")
                analysis['recommendations'].append("Consider implementing RLS if data should be restricted by user/group")
                analysis['recommendations'].append("Common RLS scenarios: regional filtering, department access, customer-specific data")

    except Exception as e:
        logger.error(f"Error generating analysis for {op_id}: {e}")
        analysis['insights'].append("Analysis generation encountered an error")

    return analysis

def handle_simple_analysis(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Fast model operations based on Microsoft Official MCP Server operations.

    Modes:
    - 'all': Run ALL 10 Microsoft MCP operations in sequence (comprehensive overview)
    - 'tables': Ultra-fast table list (< 500ms) - Microsoft MCP List operation
    - 'stats': Fast model statistics (< 1s) - Microsoft MCP GetStats operation
    - 'measures': List measures (optional table filter) - Microsoft MCP Measure List operation
    - 'measure': Get measure details (requires table + measure_name) - Microsoft MCP Measure Get operation
    - 'columns': List columns (optional table filter) - Microsoft MCP Column List operation
    - 'relationships': List relationships - Microsoft MCP Relationship List operation
    - 'partitions': List partitions (optional table filter) - Microsoft MCP Partition List operation
    - 'roles': List security roles - Microsoft MCP Role List operation
    - 'database': List databases - Microsoft MCP Database List operation
    - 'calculation_groups': List calculation groups - Microsoft MCP ListGroups operation
    """
    if not connection_state.is_connected():
        return ErrorHandler.handle_not_connected()

    agent_policy = connection_state.agent_policy
    if not agent_policy:
        return ErrorHandler.handle_manager_unavailable('agent_policy')

    # Extract mode parameter (default: all - runs all 8 auto-executable operations)
    mode = args.get('mode', 'all')

    # Special mode: Run ALL 8 auto-executable core operations (excluding partitions and measure get)
    if mode == 'all':
        import time
        start_time = time.time()

        # Initialize with prominent execution header
        execution_log = []
        execution_log.append('='*80)
        execution_log.append('>>> MICROSOFT MCP OPERATIONS - SEQUENTIAL EXECUTION <<<')
        execution_log.append('='*80)
        execution_log.append('')

        # Define all operations to execute
        operations = [
            ('01_database', 'Database Info', lambda: agent_policy.analysis_orch.list_databases_simple(connection_state)),
            ('02_stats', 'Model Statistics (GetStats)', lambda: agent_policy.analysis_orch.simple_model_analysis(connection_state)),
            ('03_tables', 'Tables List', lambda: agent_policy.analysis_orch.list_tables_simple(connection_state)),
            ('04_measures', 'Measures List (500 max)', lambda: agent_policy.analysis_orch.list_measures_simple(connection_state, None, 500)),
            ('05_columns', 'Columns List (1000 max)', lambda: agent_policy.analysis_orch.list_columns_simple(connection_state, None, 1000)),
            ('06_relationships', 'Relationships', lambda: agent_policy.analysis_orch.list_relationships_simple(connection_state, False)),
            ('07_calculation_groups', 'Calculation Groups', lambda: agent_policy.analysis_orch.list_calculation_groups_simple(connection_state)),
            ('08_roles', 'Security Roles', lambda: agent_policy.analysis_orch.list_roles_simple(connection_state)),
        ]

        operations_results = {}

        # Execute each operation with prominent visual progress
        for idx, (op_id, op_name, op_func) in enumerate(operations, 1):
            # PROMINENTLY show operation start
            execution_log.append(f'[{idx}/{len(operations)}] >>> EXECUTING: {op_name}')
            execution_log.append('-' * 80)

            op_start = time.time()
            logger.info(f'[{idx}/{len(operations)}] Executing {op_name}')

            # Execute operation
            op_result = op_func()
            op_time = round(time.time() - op_start, 3)

            # Generate operation-specific analysis summary
            op_analysis = _generate_operation_analysis(op_id, op_result)

            # Add analysis summary and timing to operation result
            op_result_enhanced = dict(op_result)
            op_result_enhanced['execution_time_seconds'] = op_time
            op_result_enhanced['analysis_summary'] = op_analysis

            # Store enhanced result
            operations_results[op_id] = op_result_enhanced

            # PROMINENTLY show operation completion with details
            if op_result.get('success'):
                # Extract key metrics from result - optimized with helper function
                msg = _extract_operation_summary(op_id, op_result)
                execution_log.append(msg)

                # Add DETAILED analysis insights to execution log - optimized
                execution_log.extend(_format_analysis_to_log(op_analysis, op_name, op_time))
            else:
                error_msg = op_result.get('error', 'Unknown error')
                execution_log.append(f"[FAIL] ERROR: {error_msg}")
                execution_log.append(f"   [TIME] Execution time: {op_time}s")
                logger.error(f'{op_name} FAILED: {error_msg}')

            execution_log.append('')  # Blank line for separation

        # Calculate total execution time
        execution_time = round(time.time() - start_time, 2)

        # Count successful operations
        successful = sum(1 for op in operations_results.values() if op.get('success'))

        # Add prominent execution summary
        execution_log.append('='*80)
        execution_log.append('EXECUTION SUMMARY')
        execution_log.append('='*80)
        execution_log.append(f'[OK] Successful operations: {successful}/{len(operations)}')
        execution_log.append(f'[TIME] Total execution time: {execution_time}s')
        execution_log.append(f'[AVG] Average time per operation: {round(execution_time/len(operations), 2)}s')
        execution_log.append('')

        # Generate detailed Power BI expert analysis
        execution_log.append('='*80)
        execution_log.append('GENERATING POWER BI EXPERT ANALYSIS')
        execution_log.append('='*80)

        temp_results = {'operations': operations_results}
        analysis = agent_policy.analysis_orch.generate_expert_analysis(temp_results)

        execution_log.append('[OK] Expert analysis complete!')
        execution_log.append('')

        # Build final response with execution log at the TOP for prominence
        # CRITICAL: Format execution_log as a string so AI displays it properly
        formatted_execution_output = '\n'.join(execution_log)

        results = {
            'success': True,
            'mode': 'all',
            'operations_count': 8,
            'successful_operations': successful,
            'execution_time_seconds': execution_time,

            # FORMATTED OUTPUT - This is what the AI should show to the user
            # Single string containing the complete operation-by-operation breakdown
            'formatted_output': formatted_execution_output,

            # IMPORTANT: Show this message FIRST to direct the AI to display formatted_output
            'message': f'Completed {successful}/{len(operations)} Microsoft MCP operations. OPERATION-BY-OPERATION RESULTS (see formatted_output below):',

            # Raw execution log (array of strings) - kept for backward compatibility
            'execution_log': execution_log,

            # Individual operation results (optimized Microsoft MCP format)
            'operations': operations_results,

            # Expert analysis
            'expert_analysis': analysis,

            # Summary (high-level only)
            'summary': f'Successfully executed {successful}/{len(operations)} operations in {execution_time}s with detailed expert analysis and operation-by-operation breakdowns'
        }

        return results

    # Route to appropriate function based on mode
    if mode == 'tables':
        result = agent_policy.analysis_orch.list_tables_simple(connection_state)
    elif mode == 'stats':
        result = agent_policy.analysis_orch.simple_model_analysis(connection_state)
    elif mode == 'measures':
        # Measure List operation
        table_name = args.get('table')
        max_results = args.get('max_results')
        result = agent_policy.analysis_orch.list_measures_simple(connection_state, table_name, max_results)
    elif mode == 'measure':
        # Measure Get operation - requires table and measure_name
        table_name = args.get('table')
        measure_name = args.get('measure_name')
        if not table_name or not measure_name:
            return {
                'success': False,
                'error': 'mode="measure" requires both table and measure_name parameters'
            }
        result = agent_policy.analysis_orch.get_measure_simple(connection_state, table_name, measure_name)
    elif mode == 'columns':
        # Column List operation
        table_name = args.get('table')
        max_results = args.get('max_results')
        result = agent_policy.analysis_orch.list_columns_simple(connection_state, table_name, max_results)
    elif mode == 'relationships':
        # Relationship List operation
        active_only = args.get('active_only', False)
        result = agent_policy.analysis_orch.list_relationships_simple(connection_state, active_only)
    elif mode == 'roles':
        # Role List operation
        result = agent_policy.analysis_orch.list_roles_simple(connection_state)
    elif mode == 'database':
        # Database List operation
        result = agent_policy.analysis_orch.list_databases_simple(connection_state)
    elif mode == 'calculation_groups':
        # Calculation Group ListGroups operation
        result = agent_policy.analysis_orch.list_calculation_groups_simple(connection_state)
    else:
        return {
            'success': False,
            'error': f'Unknown mode: {mode}. Valid modes: all, tables, stats, measures, measure, columns, relationships, roles, database, calculation_groups'
        }

    return result

def handle_full_analysis(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Unified comprehensive model analysis combining best practices, performance, and integrity.

    Formerly known as comprehensive_analysis.
    """
    if not connection_state.is_connected():
        return ErrorHandler.handle_not_connected()

    agent_policy = connection_state.agent_policy
    if not agent_policy:
        return ErrorHandler.handle_manager_unavailable('agent_policy')

    # Extract parameters with defaults
    scope = args.get('scope', 'all')
    depth = args.get('depth', 'balanced')
    include_bpa = args.get('include_bpa', True)
    include_performance = args.get('include_performance', True)
    include_integrity = args.get('include_integrity', True)
    max_seconds = args.get('max_seconds', None)

    # Run the analysis
    result = agent_policy.analysis_orch.comprehensive_analysis(
        connection_state,
        scope=scope,
        depth=depth,
        include_bpa=include_bpa,
        include_performance=include_performance,
        include_integrity=include_integrity,
        max_seconds=max_seconds
    )

    # Enrich issues with business impact context
    if result.get('success') and result.get('issues'):
        try:
            enriched_issues = []
            for issue in result['issues']:
                enriched_issue = enrich_issue_with_impact(issue)
                enriched_issues.append(enriched_issue)

            result['issues'] = enriched_issues

            # Add overall impact summary
            result = add_impact_summary(result)

        except Exception as e:
            logger.error(f"Error enriching issues with business impact: {e}", exc_info=True)
            # Don't fail the analysis if enrichment fails

    return result

def register_analysis_handlers(registry):
    """Register all analysis handlers"""
    from server.tool_schemas import TOOL_SCHEMAS

    tools = [
        ToolDefinition(
            name="06_Simple_Analysis",
            description="Fast Microsoft MCP operations with Power BI expert analysis: Runs 8 core operations (database, stats, tables, measures, columns, relationships, calculation groups, roles) + generates detailed insights and recommendations",
            handler=handle_simple_analysis,
            input_schema=TOOL_SCHEMAS.get('simple_analysis', {}),
            category="analysis",
            sort_order=60  # 06 = Analysis
        ),
        ToolDefinition(
            name="06_Full_Analysis",
            description="Comprehensive analysis: Best practices (BPA), performance, and integrity validation (10-180s)",
            handler=handle_full_analysis,
            input_schema=TOOL_SCHEMAS.get('full_analysis', {}),
            category="analysis",
            sort_order=61  # 06 = Analysis
        ),
    ]

    for tool in tools:
        registry.register(tool)

    logger.info(f"Registered {len(tools)} analysis handlers")
